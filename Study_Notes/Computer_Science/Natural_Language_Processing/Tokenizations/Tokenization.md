# Pipelines
> [!def]
> ![](Tokenization.assets/image-20240923161933439.png)
> In short, each ðŸ¤— tokenizer has their unique internal components:
> - **normalization:** General clean up, removing unwanted whitespace, casing.
> - **pre-tokenization:** Encode the raw text for downstream tokenization tasks. For example, `["How are you today"] -> [("how", (0, 3)), ("are", (3, 6)), ("you", (6, 9)), ("today", (9, 14))]`
> - **tokenization:** The actual tokenization algorithm.
> 	- Splitting the words into subwords(or characters based on algorithm type), e.g. `[("how", (0, 3)), ("are", (3, 6)), ("you", (6, 9)), ("today", (9, 14))] -> ["h", "o", "w", "u", "a", "r","e", "t", "d", "y"]`
> 	- Apply merge rules learned in order on those splits.




## Normalization
> [!def]
> The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents.TheÂ 
> 
> - The ðŸ¤— TransformersÂ `tokenizer`Â has an attribute calledÂ `backend_tokenizer`Â that provides access to the underlying tokenizer from the ðŸ¤— Tokenizers library
> 
> - `normalizer`Â attribute of theÂ `tokenizer`Â object has aÂ `normalize_str()`Â method that we can use to see how the normalization is performed
> 
> ![](Tokenization.assets/image-20240923162004534.png)


## Pre-tokenization
> [!def]
> ![](Tokenization.assets/image-20240923163057492.png)
> ä¸åŒçš„`Tokenizer`ä½¿ç”¨ä¸åŒçš„`pre-tokenization`ç­–ç•¥ï¼Œæ¯”å¦‚:
> - `Bert Tokenizer`(å‰ä¸¤ä¸ª)ï¼Œä¼šåœ¨`Normalization`æ­¥éª¤ä¸­åˆ åŽ»ç©ºæ ¼
> - `GPT2 Tokenizer`, ä¸ä¼šå±±åŒºç©ºæ ¼ï¼Œå¹¶ä¸”ä¼šåœ¨`pre-tokenization`çš„ç»“æžœä¸­å°†ç©ºæ ¼è§†ä¸º`token`çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ç”¨`G`æ¥ç¼–ç ã€‚



# ðŸ¤—Algorithms

## BPE(Byte Pair Encoding Algorithm)
### Algorithm
> [!important]
> **Reference:** https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0
> ![](Tokenization.assets/image-20240923164417694.png)![](Tokenization.assets/image-20240923164425951.png)






## WordPiece(Subword-based Tokenization Algo)
### Algorithm
> [!important]
> **Reference:** https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7
> 
> **Splitting the words:** 
> 
> ![](Tokenization.assets/image-20240923165806259.png)
> **Merge Procedures:** 
> 
> ![](Tokenization.assets/image-20240923165908186.png)


### Example
> [!example]
> ![](Tokenization.assets/image-20240923170012501.png)![](Tokenization.assets/image-20240923170024635.png)


## Unigram(Character-based Tokenization)
### Algorithm
> [!algo]






## Summary
> [!important]
> **Reference:** https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17
> ![](Tokenization.assets/image-20240923163618261.png)

