> disc2B, disc2C, disc 2D& PS4

[dis2B.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677593639290-182c2bac-6803-4ad9-87f6-f7eabd065b78.pdf)
[ans2B.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677594472444-1a34fa24-3c20-49e3-ac6c-5e0659084c37.pdf)
[dis2C.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677594472377-b4aa988f-7b5c-4a01-a56d-b3be7e7f5817.pdf)
[ans2C.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677594472348-ac88d7a9-c8c6-44e5-be0b-dd7ea50fbb7c.pdf)
[dis2D.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677679110803-aff1f31a-46bb-4db6-be3a-4cce66ad12f6.pdf)
[ans2D.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/12393765/1677679110797-5b69ca83-b2c0-4e15-ab9e-e33676ac1d19.pdf)

# Eigenvalues&Vectors
## Compute Eigenvalues
> ![image.png](3_知识点总结_练习.assets/20230305_2141012244.png)

**Solution (c) Defective Case**![image.png](3_知识点总结_练习.assets/20230305_2141011698.png)
**Solution (d) Complex Eigenvalues**![image.png](3_知识点总结_练习.assets/20230305_2141025966.png)

## nxn Eigenvalues
> ![image.png](3_知识点总结_练习.assets/20230305_2141022846.png)

**(a) Identity Matrix**因为对于$\forall x\in \mathbb{R}^n$, $Ix=x$, 所以`Identity Matrix`一定有一个特征向量为$1$，所有的$x\in \mathbb{R}^n$都是特征向量。  
**(b) Diagonal Matrix**![image.png](3_知识点总结_练习.assets/20230305_2141023386.png)
**(c) Rotation Matrix**首先意识到`Rotation Matrix`实际上是`Orthogonal Matrix`, 也就是其具有保长性。也就是$||Qx||_{\dagger}=||x||_{\dagger}$(对所有vector norms都成立)。
其次`Rotation Matrix`会将向量旋转，所以要想保证向量在旋转之后仍然和原来的向量成常数倍，矩阵的旋转角度就非常重要，这直接决定了一个旋转矩阵是否具有特征值和对应的特征向量。
我们分几种情况进行讨论:

1. 旋转角度为$0+2k\pi, k\in \mathbb{Z}$，则$Qx=x$, 此时矩阵的特征值为$1$, 特征向量为所有$\mathbb{R}^2$中的向量。
2. 旋转角度为$180+2k\pi,k\in \mathbb{Z}$, 则$Qx = -x$(也就是向量旋转之后反向)，此时矩阵的特征值为$-1$， 特征向量为所有$\mathbb{R}^2$中的向量。
3. 其他的旋转角度，没有特征向量，因为所有向量在旋转后均不能落在原向量所在的直线上。

![image.png](3_知识点总结_练习.assets/20230305_2141022058.png)
**(d) Reflection Matrix**![image.png](3_知识点总结_练习.assets/20230305_2141021142.png)![image.png](3_知识点总结_练习.assets/20230305_2141029193.png)
**(e) Matrix with zero eigenvalues**![image.png](3_知识点总结_练习.assets/20230305_2141024187.png)
**(f) Singular Matrix**![image.png](3_知识点总结_练习.assets/20230305_2141023772.png)

## Characterstic Equations
### A and Its Inverse
> ![image.png](3_知识点总结_练习.assets/20230305_2141039608.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141031742.png)


### Determinants
> ![image.png](3_知识点总结_练习.assets/20230305_2141033262.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141035384.png)


# (Un)Steady State
## P1 Steady State Reservoir
> ![image.png](3_知识点总结_练习.assets/20230305_2141037094.png)

**(a)**![image.png](3_知识点总结_练习.assets/20230305_2141032452.png)
**(b)**![image.png](3_知识点总结_练习.assets/20230305_2141031184.png)![image.png](3_知识点总结_练习.assets/20230305_2141043785.png)


## P2 Identify Steady States⭐⭐⭐⭐⭐
> ![image.png](3_知识点总结_练习.assets/20230305_2141047430.png)

**Solution**本题可以走捷径，首先我们知道，如果一个`Transition`想要有`Steady State`，必须存在$\lambda=1$, 我们可以检验$det(A-1\cdot I)=0$是否成立。
![image.png](3_知识点总结_练习.assets/20230305_2141049704.png)![image.png](3_知识点总结_练习.assets/20230305_2141046156.png)
发现确实存在$\lambda=1$, 但是这还不够，我们需要检验是否所有的`Eigenvalues`都属于$-1<\lambda_i\leq 1$这个区间内。
我们通过`numpy`计算出来确实是的$\lambda_1=1, \lambda_2 = 0.5, \lambda_3 = -\frac{1}{3}$。所以我们的`Steady State Vector`一定有$\begin{bmatrix} 1\\2\\1\end{bmatrix}$的形式。但是我们知道，任何这个向量的常数倍向量也都是`Steady State Vectors`(因为他们同属于$\lambda=1$的特征空间, 所以都是特征向量，都满足$Ax=1\cdot x$)。
然后我们要根据`Initial State`来判断最终的`State`是什么，由于我们的状态转移矩阵是`Conservative`的(因为各列和为$1$。)，所以最终的`Steady State`向量的各项和应该等于$300$, 所以按比例分配得到:
![image.png](3_知识点总结_练习.assets/20230305_2141048042.png)


## P3 Unsteady States
> ![image.png](3_知识点总结_练习.assets/20230305_2141041865.png)![image.png](3_知识点总结_练习.assets/20230305_2141049557.png)

**Solution (a)**![image.png](3_知识点总结_练习.assets/20230305_2141048038.png)
**Solution (b)**![image.png](3_知识点总结_练习.assets/20230305_2141048048.png)


# Change of Basis&Diagonalization
## Idenfity Diagonalization
> ![image.png](3_知识点总结_练习.assets/20230305_2141054439.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141057792.png)





## Linear Trans. Matrix⭐⭐⭐⭐
> ![image.png](3_知识点总结_练习.assets/20230305_2141057232.png)![image.png](3_知识点总结_练习.assets/20230305_2141055974.png)
> 本题非常经典，我们详细分析一下:
> $(a)$: 我们凭直觉就可以给出$\{t^2,t,1\}$这一组基，因为他们之间互相线性无关，且他们的线性组合$at^2+bt+c$就可以表示所有的$\mathbb{P}_2$。
> $(b)$: 回顾我们在`Change of Basis`中讲解的，对于一个线性变换$T\vec{u}=\vec{v}$来说，$\vec{u}$和$\vec{v}$都是在标准基下的坐标表示。我们的目标是求出矩阵$T$。同时我们知道，如果$\vec{u}=\begin{bmatrix} 1\\0\\0\end{bmatrix}$, 则$T\vec{u}$得到的就是矩阵$T$的第一列。同理如果$\vec{u}=\begin{bmatrix} 0\\1\\0\end{bmatrix}$, 则$T\vec{u}$得到的就是矩阵$T$的第二列，如果$\vec{u}=\begin{bmatrix} 0\\0\\1\end{bmatrix}$, 则$T\vec{u}$得到的就是矩阵$T$的第三列$T$。而我们知道$\vec{u}$其实是一种坐标表示。在本题中，$\vec{u}$可以理解成任意$\mathbb{P}_2$在$\{t^2,t,1\}$(多项式的标准基)下的坐标表示，所以如果$\vec{u}=\begin{bmatrix} 1\\0\\0\end{bmatrix}$， 则对应的输入多项式就是$f(t)=1\cdot t^2+0\cdot t+0\cdot 1=t^2$， 而我们知道$T_1(f(t))=2f(t)$，所以$T_1(t^2)=2t^2$, 用标准基表示为$\begin{bmatrix} 2\\0\\0\end{bmatrix}$, 于是我们知道了$T_1$的第一列就是$\begin{bmatrix} 2\\0\\0\end{bmatrix}$。同理我们将$\vec{u}=\begin{bmatrix} 0\\1\\0\end{bmatrix}$,$\vec{u}=\begin{bmatrix} 0\\0\\1\end{bmatrix}$都按照前面的步骤走一遍，分别得到$T_1$的第二列和第三列是$\begin{bmatrix} 0\\2\\0\end{bmatrix}$, $\begin{bmatrix} 0\\0\\2\end{bmatrix}$, 所以最终的结果是$T_1=\begin{bmatrix} 2&0&0\\0&2&0\\0&0&2\end{bmatrix}$。
> 第二小问同理，我们可以得到$T_2=\begin{bmatrix}0&0&0\\2&0&0\\0&1&0 \end{bmatrix}$
> $(c)$: 本题有**两种视角看待**:
> 1. 把$\vec{x_0}, \vec{x_1},\vec{x_2}$看成**三个待求解的变量**: 则我们直接根据题目条件得到方程组:$\begin{bmatrix}1&1&1\\1&0&-1\\0&2&0 \end{bmatrix}\begin{bmatrix} x_0\\x_1\\x_2\end{bmatrix}=\begin{bmatrix}2t^2+3t\\t+1\\4t+2 \end{bmatrix}$, 然后求解即可，得到$x_0=t^2+t$, $x_1=2t+1$, $x_2=t^2-1$.
> 2. 把$\vec{x_0}, \vec{x_1}, \vec{x_2}$看成是**一组基**。则根据`Change of Basis`的讲义我们知道，假设对同一个向量$\vec{u}$, 我们有两种坐标表示，不妨令其中一种是标准基，也就是$\{t^2,t,1\}$, 另一种是$\{x_0, x_1,x_2\}$。所以我们会有: $1\cdot \vec{x_0}+1\cdot \vec{x_1}+1\cdot \vec{x_2}=2\cdot t^2+3\cdot t+0\cdot 1$, 也就是说我们可以写成$\begin{bmatrix} \vec{x_0}&\vec{x_1}&\vec{x_2} \end{bmatrix}\begin{bmatrix} 1&1&0\\1&0&2\\1&-1&0\end{bmatrix}=\begin{bmatrix} 2&0&0\\3&1&4\\0&1&2\end{bmatrix}$, 然后通过矩阵的逆求解得到$\vec{x_0},\vec{x_1},\vec{x_2}$在标准基$\{t^2,t,1\}$ 下的表示，得到$\begin{bmatrix} 1&0&1\\1&2&0\\0&1&-1\end{bmatrix}$ 。按列表示成多项式可得$x_0=t^2+t, x_1=2t+1, x_2=t^2-1$。




# 综合题
## Aragorn's Odyssey
### Problem Settings
> ![image.png](3_知识点总结_练习.assets/20230305_2141051124.png)



### (a) Rotation Matrix
> ![image.png](3_知识点总结_练习.assets/20230305_2141055288.png)![image.png](3_知识点总结_练习.assets/20230305_2141055663.png)
> 本题比较简单，就是注意先关于$x$轴翻转，对应的矩阵是$\begin{bmatrix} 1&0\\0&-1\end{bmatrix}$。然后就是熟悉的旋转矩阵。



### (b) Orthogonal Invariant
> ![image.png](3_知识点总结_练习.assets/20230305_2141058439.png)![image.png](3_知识点总结_练习.assets/20230305_2141052843.png)
> 本质上，翻转和旋转都不会改变被操作向量的长度。于是本来长度是$1$, 之后长度也是$1$。



### (c) Eigenvectors
> ![image.png](3_知识点总结_练习.assets/20230305_2141064073.png)![image.png](3_知识点总结_练习.assets/20230305_2141068780.png)



### (d) Subspace Dimension 
> ![image.png](3_知识点总结_练习.assets/20230305_2141066460.png)![image.png](3_知识点总结_练习.assets/20230305_2141066443.png)



## Trouble in Telecomm
### Problem Settings
> ![image.png](3_知识点总结_练习.assets/20230305_2141061270.png)


 
### (a) Null Spaces
> ![image.png](3_知识点总结_练习.assets/20230305_2141063064.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141066964.png)


### (b) Non-Invertibility
> ![image.png](3_知识点总结_练习.assets/20230305_2141064251.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141079271.png)

### (c) Invertibility
> ![image.png](3_知识点总结_练习.assets/20230305_2141071248.png)

**Solution**![image.png](3_知识点总结_练习.assets/20230305_2141076800.png)![image.png](3_知识点总结_练习.assets/20230305_2141077508.png)![image.png](3_知识点总结_练习.assets/20230305_2141075886.png)


# Summary
## Eigenvalues&Vector Basics⭐⭐⭐⭐⭐
> 对于任意矩阵$A$, 我们通过$det(\lambda\mathbf{I}-A)=0$求出特征值:
> 1. 每个特征值对应一个特征空间，由通过零空间解出的线性无关的特征向量`span`而成。
> 2. 每个特征值对应的特征空间中的所有向量都称为特征向量。
> 3. 一个特征值可以对应多个特征向量，这些特征向量组成特征空间，特征空间是一个子空间(满足加法数乘封闭性，且零向量在特征空间中)
> 4. 一个特征向量只能对应一个特征值，因为特征向量在矩阵左乘之后只能放大固定的倍数。因此如果存在$x_1$, 使得$Ax_1=\lambda_1x_1$和$Ax_2 = \lambda_2x_2$同时成立，则$\lambda_1 = \lambda_2$。

 

## Eigenvalues for Special Matrices
> 1. `**Rotation Matrix**`$\mathbf{R}_{2\times 2}$**不一定存在特征值**，当旋转角度为$0+2k\pi, \pi+2k\pi, k\in \mathbb{Z}$时，存在特征值，分别为$1$和$-1$, 特征向量为$\mathbb{R}^2$(特征空间中所有的向量都满足$\mathbf{Rx}=x$或者$\mathbf{Rx=-x}$)
> 2. `**Reflection Matrix**`$\mathbf{R}_{2\times 2}$**一定有两个特征值，分别为**$1$**和**$-1$。假设我们的`Reflection Matrix`为$\begin{bmatrix} cos\theta &sin\theta \\ sin\theta&-cos\theta\end{bmatrix}$, 则我们的对翻转称轴与$x$轴所成的角度是$\frac{\theta}{2}$。
>    1. 此时对于所有落在翻转对称轴上的向量$x_{ref}$来说，矩阵操作对其没有效果，也就是说$\mathbf{R}x=x$。特征空间为$span\{x_{ref}\}$。
>    2. 对于所有垂直于翻转对称轴上的向量$x_{perp}$来说，矩阵操作对其的效果就是`flip it`, 也就是说$\mathbf{R}x=-x$。特征空间为$span\{x_{perp}\}$。
>    3. 总的来说，特征空间就是对称轴和垂直于对称轴的方向组成的$\mathbb{R}^2$。
> 3. **对于不可逆的矩阵，一定存在一个特征值为零。**我们可以这样想，一个不可逆矩阵其零空间一定除了$\{0\}$以外还有其他向量，而零空间对应的是$0$这个特征值对应的特征空间。而某个特征值存在的充要条件就是$A-\lambda I$不可逆（存在`Non Trivial Solutions`）。因为$A$不可逆，所以$\lambda=0$时，$A-\lambda I=A$不可逆，所以一定存在$0$特征值。



## Eigenvalues for Steady State
### Initial State is Eigenvectors
> ![image.png](3_知识点总结_练习.assets/20230305_2141072390.png)![image.png](3_知识点总结_练习.assets/20230305_2141073816.png)



### General Initial States
> ![image.png](3_知识点总结_练习.assets/20230305_2141075045.png)![image.png](3_知识点总结_练习.assets/20230305_2141074774.png)![image.png](3_知识点总结_练习.assets/20230305_2141084090.png)




## Characteristic Equation
> 对于矩阵$A$来说，假设我们知道其存在一个特征值$\lambda$满足$Ax=\lambda x$, 则$p(A)x=p(\lambda)x$始终成立。
> 其中$A^{-1}$可以看作$\frac{1}{A}$, 对应特征值$\frac{1}{\lambda}$($\lambda\neq 0$, 即$A$可逆)。

 


## Change of Basis&Diagonalization
> 1. **坐标变换: **对于一个向量$\vec{u}$来说，一般而言我们假设这是在`Standard Basis`$\vec{e_1},\cdots, \vec{e_n}$下的坐标表示，如果我们想换一个`Basis`，比如换成$\vec{a_1},\cdots, \vec{a_n}$, 则我们可以通过解线性方程组$A\vec{u_a}=\vec{u}$得到向量$\vec{u}$在$\vec{a_1}\cdots, \vec{a_n}$下的坐标表示$\vec{u_a}=A^{-1}\vec{u}$。如果想进行相反的操作，则$A\vec{u_a}=\vec{u}$即可。
> 
![image.png](3_知识点总结_练习.assets/20230305_2141087710.png)
> 2. **坐标变换下的线性变换：**对于一个线性变换$T$来说，在`Standard Basis`下，我们一般使用$T\vec{u}=\vec{v}$表示一个$\mathbb{R}^n\to\mathbb{R}^n$的`Mapping`。此时我们可以将$T$写成$AT_aA^{-1}$, 表示先将$\vec{u}$转换到$\vec{a_i}$的这个`Basis`, 然后在这个`Basis`下使用对应的线性变换$T_a$, 然后将得到的结果通过左乘$A$转换回`Standard Basis`中。
> 3. **在特征空间下进行线性变换: **对于一个线性变换$T$来说，我们也可以选取一些性质较好的`Basis`, 比如说$T$的`Eigenvectors`组成的`Basis`, 此时$T$可以写成$ADA^{-1}$, 其中$D$是一个`Scalar Matrix`, 也就是我们先通过$A^{-1}$将其转换到`Eigenspace`中，然后在`Eigenspace`中的线性变换实际上就是拉伸变换，最后左乘一个$A$复原即可。
> 4. **矩阵的幂: **对于一个矩阵$T_{n\times n}$来说(必须是方阵)，如果他有$n$**个线性无关的特征向量, **则可以进行对角化操作，写成$T=ADA^{-1}$的形式，此时如果我们要计算矩阵的幂就非常简单，$T^k=AD^kA^{-1}$。



## Linear Transformation Matrix
> 假设现在我们有一个线性变换$T\vec{u}=\vec{v}$, 同时我们又知道了这个线性变换在给定输入情况下的某些输出，那么我们实际上就可以确定这个线性变换$T$对应的矩阵。具体方法在上面的`Linear Transformation Matrix`题中有介绍。

