**参考:**[**https://zhuanlan.zhihu.com/p/46100282**](https://zhuanlan.zhihu.com/p/46100282)
# 0 前言
> 本章我们介绍奇异矩阵，并介绍其在图像处理中的应用。
> **任意矩阵**可分解为 $\bf A=UΣV^T$ ，分解结果为正交矩阵$\bf U$，对角阵$\bf Σ$和正交矩阵$\bf V$。
> 如果矩阵$\bf A$是正定矩阵，它的奇异值分解就是$\bf A=QΛQ^T$，一个正交矩阵$\bf Q$就可以满足分解，而不需要两个, 此时$\bf U=V=Q$, 是`SVD`的特殊情况。
> 而对于可对角化的方阵（有$n$个线性无关的特征向量）有$\bf A=SΛS^{−1}$，但特征向量矩阵$\bf S$并不是正交矩阵，而`SVD`中的$\bf U$和$\bf V$都是正交矩阵。


# 1 奇异值分解干了啥？
## 1.1 为什么要有U和V
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025015643.png)
> 对于任意一个$m\times n$的矩阵$\bf A$, 其秩为$r$， 我们想要对其进行对角化操作，但是不会写成$\bf SDS^{-1}$的形式。因为$\bf S$组成的特征向量矩阵有三个致命问题:
> 1. 特征向量彼此不正交（也就是$\bf S$不是正交矩阵），而我们知道正交有很多非常好的性质, 可以简化运算。
> 2. 特征向量数量可能不够导致矩阵根本无法对角化，也就是$\bf SDS^{-1}$的分解根本不存在。
> 3. 对角化操作要求矩阵是方阵，而我们希望对于任意长方形矩阵$\bf A$都能被分解。
> 
`SVD`解决了这个问题，使得对于任何矩阵$\bf A$, 都能够写成$\bf A=U\Sigma V^T$, $\bf U,V$均为正交矩阵(含有标准正交列的矩阵，不一定是方阵)。
> **在奇异值分解的框架**下，我们称特征向量为**奇异向量**。不同于特征值分解的时候我们只需要待分解矩阵$\bf A$的一组特征向量，**奇异向量一般有两组**, 标记为$\mathbf u's$和$\mathbf v's$
> $\bf u_i\in R^m,v_i\in R^n$, $\bf u_i$作为$\bf U$的列向量，$\bf v_i$作为$\bf V$的列向量。
> 因此$\bf U\in R^{m\times m},V\in R^{n\times n}$，$\bf U,V$**都是正交方阵。**



## 1.2 U,V,Σ都包括些什么
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025015518.png)
> **做奇异值分解就是在矩阵的四个子空间中寻找到合适的基：**
> `"A is diagonalized"`在本章中指的是矩阵$\bf A$的奇异值分解。其中奇异值$\sigma_i$都是正数，且代表的是$\bf Av_i$的长度。这些奇异值最终都去了$\bf \Sigma$矩阵中。
> 我们可以将矩阵$\bf A$视为一种线性变换操作，将其行空间中的一个向量$\bf v_1$,变为其列空间中的向量 $\bf u_1=Av_1$ 。奇异值分解就是要在行空间中寻找一组正交基，将其通过矩阵$\bf A$线性变换生成列空间中的一组正交基 $\bf Av_i=σ_iu_i$ 。
> - $\bf v_1,v_2…,v_r$ 为**行空间**的标准正交基。
> - $\bf u_1,u_2…,u_r$ 为**列空间**的标准正交基。
> - $\bf v_{r+1},v_{r+2}…,v_n$ 为**零空间**的标准正交基。
> - $\bf u_{r+1},u_{r+2}…,u_m$为**左零空间**的标准正交基。

**图解**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025021179.png)

## 1.3 U,V,Σ的构建过程
> 我们令$\bf u_1,u_2…,u_r$组成$\bf U_r\in R^{m\times r}$, 令$\bf v_1,v_2…,v_r$组成$\bf V_r\in R^{n\times r}$
> 根据$(1)$中的关系得到$\bf AV_r=U_rZ_r$
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025022685.png)
> $(2)$式是奇异值分解的核心，从前文我们知道，$\bf U\in R^{m\times m}$, 但是现在的$\bf U_r\in R^{m\times n}$,所以如果我们想要扩充$\bf U_r$, 就需要增加更多的和$\bf U_r$中的列向量正交的向量，形成$\bf U$
> - 由于$\bf u_1,...,u_r$是$\bf A$列空间的标准正交基，而我们从矩阵的[四大基本空间](https://www.yuque.com/alexman/so5y8g/xhq2kv)知道，矩阵的列空间中的向量和左零空间中的向量正交，**所以我们选择将**$\bf A$**的左零空间的标准正交基加入**$\bf U_r$**中**，最终得到的$\bf U_{m\times m}=\begin{bmatrix} \bf u_1&\bf u_2&\cdots&\bf u_r&\bf u_{r+1}&\cdots&\bf u_m\end{bmatrix}$是一个正交方阵。
> - 同理由于$\bf v_1,...,v_r$是$\bf A$行空间的标准正交基，**所以我们选择将**$\bf A$**的零空间的标准正交基加入**$\bf V_r$**中**，最终得到的$\bf V_{n\times n}=\begin{bmatrix} \bf v_1&\bf v_2&\cdots&\bf v_r&\bf v_{r+1}&\cdots&\bf v_m\end{bmatrix}$是一个正交方阵。
> 
最终我们有如下矩阵表达式$\bf A_{m\times n}V_{n\times n}=U_{m\times m}\Sigma_{m\times n}$的展开式:$\mathbf{A}\begin{bmatrix} \mathbf{v_1}&\mathbf{v_2}&\cdots&\mathbf{v_r}&\mathbf{v_{r+1}}\cdots \mathbf{v_n}\end{bmatrix}=\begin{bmatrix} \mathbf{u_1}&\mathbf{u_2}&\cdots&\mathbf{u_r}&\mathbf{u_{r+1}}\cdots\mathbf{u_n}\end{bmatrix}\begin{bmatrix} \bf \Lambda_{r\times r}&\bf 0\\\bf 0&\bf 0_{(m-r)\times (n-r)}\end{bmatrix}$
> 此时我们得到的$\bf U_{m\times m}$和$\bf V_{n\times n}$都是正交矩阵。因此满足$\bf U^{-1}=U^T$, $\bf V^{-1}=V^T$
> 于是$\bf AV=U\Sigma$两边同乘以$\bf V^{-1}$得到$\bf A=U\Sigma V^{-1}=U\Sigma V^T$  , `SVD`完成。

> 找出矩阵$\bf A$行空间中的正交基很容易，`Gram-Schmidt`正交化过程就可以做到，**但是随便的一组正交基经过矩阵矩阵**$\bf A$**变换得到的向量并不一定正交，**因此满足此要求的行空间的正交基非常特殊。而矩阵$\bf A$零空间的向量所对应的是矩阵$\bf Σ$对角线上的$0$元素，因此很容易处理。


# 2 奇异值分解的意义
## 2.1 奇异值分解的三种形式
> 聊完了奇异值分解的各个组成部分都是什么，怎么得到的之后，我们来探究`SVD`的意义。

> 还记得上文章提到的展开式:
> $\mathbf{A}\begin{bmatrix} \mathbf{v_1}&\mathbf{v_2}&\cdots&\mathbf{v_r}&\mathbf{v_{r+1}}\cdots \mathbf{v_n}\end{bmatrix}=\begin{bmatrix} \mathbf{u_1}&\mathbf{u_2}&\cdots&\mathbf{u_r}&\mathbf{u_{r+1}}\cdots\mathbf{u_n}\end{bmatrix}\begin{bmatrix} \bf \Lambda_{r\times r}&\bf 0\\\bf 0&\bf 0_{(m-r)\times (n-r)}\end{bmatrix}$
> 我们对其进行变形为$\bf A=U\Sigma V^{-1}=U\Sigma V^T$的形式:
> $\mathbf{A}=\begin{bmatrix} \mathbf{u_1}&\mathbf{u_2}&\cdots&\mathbf{u_r}&\mathbf{u_{r+1}}\cdots\mathbf{u_n}\end{bmatrix}\begin{bmatrix} \bf \Lambda_{r\times r}&\bf 0\\\bf 0&\bf 0_{(m-r)\times (n-r)}\end{bmatrix}\begin{bmatrix} -\mathbf{v_1}-\\-\mathbf{v_2}-\\\vdots\\-\mathbf{v_r}-\\-\mathbf{v_{r+1}}-\\\vdots\\- \mathbf{v_n}-\end{bmatrix}\cdots(3)$
> 然后回忆一下矩阵乘法的[列乘以行](https://www.yuque.com/alexman/so5y8g/gkg1kx#rYK66)视角，我们可以将$(3)$变为:
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025025493.png)
> 对应了下面的图:
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025026675.png)
> **至此我们针对**`**SVD**`**有如下三种写法**


### Reduced SVD
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025025265.png)


### Full SVD
> $\mathbf{A}\begin{bmatrix} \mathbf{v_1}&\mathbf{v_2}&\cdots&\mathbf{v_r}&\mathbf{v_{r+1}}\cdots \mathbf{v_n}\end{bmatrix}=\begin{bmatrix} \mathbf{u_1}&\mathbf{u_2}&\cdots&\mathbf{u_r}&\mathbf{u_{r+1}}\cdots\mathbf{u_m}\end{bmatrix}\begin{bmatrix} \bf \Lambda_{r\times r}&\bf 0\\\bf 0&\bf 0_{(m-r)\times (n-r)}\end{bmatrix}$


### Rank One Decomposition
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025022485.png)![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025022160.png)



## 2.2 SVD&Eigen-Decom
### 奇异值和特征值⭐⭐⭐⭐⭐
> **首先我们要明确几点:**
> - 奇异值永远是非负的
> - 特征值可以是负的
> 
我们在对$\mathbf{A}$进行奇异值分解的时候， 都会先求$\mathbf{A^TA}$的特征值，而我们知道$\mathbf{A^TA}$是一个性质很好的矩阵, 他有如下两个重要性质:
> 1. $Rank(\mathbf{A^TA})=Rank(\mathbf{A})$
> 2. $\mathbf{A^TA}$是半正定的，因为$\mathbf{x^TA^TAx\geq 0}$(看成向量的长度)， 所以其特征值均非负。
> 
奇异值的求法就是通过对$\mathbf{A^TA}$的特征值开根号，因为其特征值非负，所以奇异值存在且一定非负。


### When are they the same?
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025039556.png)
> **注意为什么一定要是**`**Semi**`**或者**`**Positive Definite**`**呢?**
> 还记得奇异值分解中的$\bf \Sigma$中的`Diagonal Entries`代表的都是$\bf A$的列空间的正交基的长度吗，长度必须大于等于零对吧。所以当$\bf \Sigma=\Lambda$时，$\bf \Lambda$中的每一项都必须大于等于零。从特征值视角来看，特征值大于等于零的矩阵不就是半正定矩阵嘛。


### Rank One Decomposition
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025033256.png)



## 2.3 总结
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025039837.png)

# 3 如何寻找SVD分解矩阵
## 3.1 使用A^TA和AA^T
### 奇异值定理
> 一个矩阵$\bf A$的奇异值就相当于$\bf A^TA$或者$\bf AA^T$的特征值的**平方根(**因为前两个矩阵都是半正定矩阵， 所以特征值为非负，所以奇异值恒非负**)**。
> - 矩阵$\bf A$有两组奇异向量，分别对应与$\bf A^TA$的标准正交特征向量和$\bf  AA^T$的标准正交特征向量。
> - 同时，矩阵$\bf A$只有一组奇异值，因为$\bf A^TA$的特征值和$\bf AA^T$的**正特征值**相同，我们给出[代数视角证明](https://math.stackexchange.com/questions/2672419/let-a-be-an-m-times-n-matrix-show-ata-and-aat-have-the-same-eigenval)
> 
尽管$\bf A$一般不是方阵，但是$\bf A^TA$或者$\bf AA^T$都是方阵且对称。


### 寻找SVD分解
> 现在的问题就是怎么找到符合要求的向量$\bf v_i$和$\bf u_i$, 即正交矩阵$\bf V$和$\bf U$。
> 为了得到他们，我们考虑首先解决其中的一个$\bf V$，在等式$\bf A=UΣV^T$ 两侧分别乘上等式$\bf A^T=VΣ^TU^T$ 两侧的项：
> $\bf A^TA=VΣ^TU^TUΣV^T=VΣ^TΣV^T=V\begin{bmatrix} \sigma_1^2&&&\\&\sigma_2^2\\&&\\&&\ddots&\\&&&\sigma_r^2\end{bmatrix}V^T$
> 上式其实是正定(或者半正定)矩阵 $\bf A^TA$ 的正交分解，$\bf v_i$就是矩阵$\bf A^TA$的特征向量， $\bf σ_i^2$ 就是矩阵$\bf A^TA$ 的特征值，奇异值 $\bf σ_i$ 要取正平方根。$\bf V$的列向量$\bf v_i$就是矩阵$\bf A^TA$ 的的特征向量。
> 
> 用同样的办法也可以求得$\bf U$。$\bf AA^T=UΣV^TVΣ^TU^T=UΣ^TΣU^T=U\begin{bmatrix} \sigma_1'^2&&&\\&\sigma_2'^2\\&&\\&&\ddots&\\&&&\sigma_r'^2\end{bmatrix}U^T$
> $\bf U$的列向量$\bf u_i$就是矩阵$\bf AA^T$ 的的特征向量。
> 
> 特征值分解成功的原因如下：
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025032599.png)


###  求解SVD案例
#### 2x2案例 1
> 求矩阵$\bf A= \begin{bmatrix} 4&4\\-3&3\end{bmatrix}$的奇异值分解$\bf A=U\Sigma V^T$。
> 1. **首先我们求**$\bf V$**, 相当于求**$\bf A^TA$**的特征向量**$\bf v_i$**。**
> 
$\bf A^TA=\begin{bmatrix} 4&4\\-3&3\end{bmatrix}\begin{bmatrix} 4&-3\\4&3\end{bmatrix}=\begin{bmatrix} 25&7\\7&25\end{bmatrix}$, 特征多项式是$\lambda^2-50\lambda+576=0$, 	所以$\sigma_1^2=32,\sigma_2^2=18$, 对应的特征向量是$\bf v_1=\begin{bmatrix} 1\\1\end{bmatrix}$和$\bf v_2=\begin{bmatrix} 1\\-1\end{bmatrix}$, 因为我们要的是正交矩阵，所以我们将这两个彼此正交的特征向量(因为$\bf A^TA$的对称矩阵) 进行标准化得到正交矩阵$\bf V=\frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix}$
> 2. **然后我们求**$\bf U$**, 相当于求**$\bf AA^T$**的特征向量**$\bf u_i$**。**
> 
$\bf AA^T=\begin{bmatrix} 4&-3\\4&3\end{bmatrix}\begin{bmatrix} 4&4\\-3&3\end{bmatrix}=\begin{bmatrix} 32&0\\0&18\end{bmatrix}$, 是上三角矩阵，所以$\sigma_1^2=32,\sigma_2^2=18$, 对应的特征向量是$\bf u_1=\begin{bmatrix} 1\\0\end{bmatrix}$和$\bf u_2=\begin{bmatrix} 0\\1\end{bmatrix}$, 因为我们要的是正交矩阵，所以我们将这两个彼此正交的特征向量(因为$\bf A^TA$的对称矩阵) 进行标准化得到正交矩阵$\bf U=\begin{bmatrix}1&0\\0&1\end{bmatrix}$
> 3. 验证$\bf Av=u$的合法性
> 
根据我们在`1,2`中得到的矩阵$\bf U,V$, 我们发现，$\mathbf{Av_1}=\begin{bmatrix} 4&4\\-3&3\end{bmatrix}\begin{bmatrix} 1/\sqrt{2}\\1/\sqrt{2}\end{bmatrix}=\begin{bmatrix} 4\sqrt{2}\\0\end{bmatrix}\neq\begin{bmatrix} 1\\0\end{bmatrix}$, 但这还好，$\bf Av_1$和$\bf u_1$只是大小不同，方向至少还是一致的。但是对于$\mathbf{Av_2}=\begin{bmatrix} 4&4\\-3&3\end{bmatrix}\begin{bmatrix} 1/\sqrt{2}\\-1/\sqrt{2}\end{bmatrix}=\begin{bmatrix} 0\\-6\sqrt{2}\end{bmatrix}\neq\begin{bmatrix} 0\\1\end{bmatrix}$，这下好了，$\bf Av_2$和$\bf u_2$只是大小方向都不同。这是因为特征向量不是唯一的， 因为对任意矩阵而言，特征值的方向所在的直线 (称为轴, 这我们之前在[主轴定理](https://www.yuque.com/alexman/so5y8g/oiz47y#gdvRl)中介绍过，对于一个对称矩阵而言，其特征值的方向所在的直线就是我们的主轴，之所以称为轴，是因为不同特征值的特征向量正交， 因为坐标轴也正交嘛) 上的所有向量都可以是特征向量，方向不同也可以，所以就造成了上面的情况。再者说，$\bf u$** **和$\bf v$之间的符号联系在进行$\bf AA^T$的计算时被切断了，而用$\bf AV=UΣ$可以避免此问题。
> 4. 标准求法
> 
其实根据$\bf v_i$和$\bf u_i$的关系$\bf Av_i=u_i$, 一旦我们求出了$\bf v_i$，$\bf u_i$也就随之确定，所以我们其实不需要进行第`2`步。

 
####  2x2案例 2
> 假设我们有矩阵$\bf A=\begin{bmatrix} 4&3\\8&6\end{bmatrix}$, 求其奇异值分解。
> 还记得我们讲过的:
> $\bf AV=UΣ$, 如下所示:
> $\mathbf{A}\begin{bmatrix} \mathbf{v_1}&\mathbf{v_2}&\cdots&\mathbf{v_r}&\mathbf{v_{r+1}}\cdots \mathbf{v_n}\end{bmatrix}=\begin{bmatrix} \mathbf{u_1}&\mathbf{u_2}&\cdots&\mathbf{u_r}&\mathbf{u_{r+1}}\cdots\mathbf{u_n}\end{bmatrix}\begin{bmatrix} \sigma_1&&&\\&\sigma_2\\&&\\&&\ddots&\\&&&\sigma_r\\&&&&\sigma_{r+1}\\&&&&&\\&&&&&\ddots\\&&&&&&\sigma_{n}\end{bmatrix}$
> **列空间**对应的正交基是$\begin{bmatrix} \bf v_1,v_2,...,v_r\end{bmatrix}$
> **零空间**对应的正交基是 $\begin{bmatrix}\mathbf{v_{r+1}}\cdots \mathbf{v_n} \end{bmatrix}$ 
> **行空间**对应$\begin{bmatrix} \bf u_1,u_2,...,u_r\end{bmatrix}$  
> **左零空间**对应的正交基是$\begin{bmatrix}\mathbf{u_{r+1}}\cdots \mathbf{u_n} \end{bmatrix}$
> 
> 注意到这个矩阵是`Singular Matrix`, 所以其行秩和列秩为$1$(主元个数为$1$), 所以其零空间和左零空间的秩也是$1$。
> **很容易确定:**
> - 行空间正交基是$\{\bf v_1\}$,$\bf v_1= \begin{bmatrix} 0.8\\0.6\end{bmatrix}$ 
> - 零空间正交基是$\{\bf v_2\}$,$\bf v_2= \begin{bmatrix} 0.6\\-0.8\end{bmatrix}$ ，
> - 列空间的正交基是$\{\bf u_1\}$,$\bf u_1= \frac{1}{\sqrt{5}}\begin{bmatrix} 1\\2\end{bmatrix}$
> - 列空间的正交基是$\bf \{u_2\}$,$\bf u_2= \frac{1}{\sqrt{5}}\begin{bmatrix} 2\\-1\end{bmatrix}$ 。
> 
$\bf A^TA = \begin{bmatrix} 4&8\\3&6\end{bmatrix}\begin{bmatrix} 4&8\\8&6\end{bmatrix}=\begin{bmatrix} 80&60\\60&45\end{bmatrix}$ ，秩$1$矩阵，很容易求得$\bf σ_1^2=125，σ_2^2=0$。
> 矩阵的`**SVD**`分解为：$\bf A= \frac{1}{\sqrt{5}} \begin{bmatrix} 1&2\\2&-1\end{bmatrix}\begin{bmatrix} \sqrt{125}&0\\0&0\end{bmatrix}\begin{bmatrix} 0.8&0.6\\0.6&-0.8\end{bmatrix}$


## 3.2 使用一个对称矩阵
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025039030.png)



## 3.3 总结
> 奇异值分解在最小二乘法问题中有重要应用，因为在实际问题中常碰到矩阵$\bf A$不是列满秩的状态，因此$\bf A^TA$ 不可逆，无法用之前的方法求最优解。即使是列满秩的情况当矩阵是超大型矩阵时，$\bf A^TA$ 的计算量太大，用奇异值分解的办法会降低计算量。




# 4 SVD 的几何意义
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025042250.png)
> 本小节给出`SVD`的几何意义，对后续图像处理的理解有帮助。图来自原书第五版。
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025048448.png)



# 5 秩1展开
> 本小节探讨一个使用`SVD`的秩`1`展开带来的好处和应用。

## 5.1 算例
> 假设现在有这样一个奇怪的矩阵：
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025044715.png)
> $\bf A^TA=\begin{bmatrix} 0&0&0&0\\0&1&0&0\\0&0&4&0\\0&0&0&9\end{bmatrix}$,$\bf AA^T=\begin{bmatrix} 1&0&0&0\\0&4&0&0\\0&0&9&0\\0&0&0&0\end{bmatrix}$
> 由于都是上三角矩阵，所以对角线上的元素是其特征值。由[3.1](https://www.yuque.com/alexman/so5y8g/psybsc/edit#YxFKw)中可知, $\bf A^TA$和$\bf AA^T$的正特征值相同。然后我们求特征向量，$\bf u$对应$\bf A^TA$,$\bf v$对应$\bf AA^T$
> $\bf U=\begin{bmatrix} 0&0&1&0\\0&1&0&0\\1&0&0&0\\0&0&0&1 \end{bmatrix}$, $\bf \Sigma=\begin{bmatrix} 3&0&0&0\\0&2&0&0\\0&0&1&0\\0&0&0&0 \end{bmatrix}$, $\bf V=\begin{bmatrix} 0&0&0&1\\0&0&1&0\\0&1&0&0\\1&0&0&0 \end{bmatrix}$
> $\sigma_1^2=9,\sigma_2^2=4,\sigma_3^2=1$（$\sigma_i$为奇异值, $\sigma_i$为特征值）
> 注意到$\bf U$和$\bf V$之间只差了一个$4\times 4$的置换矩阵。
> 根据下面的`Rank One Version of SVD`:![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025046705.png)
> 我们发现$\mathbf{u_1}\sigma_1\mathbf{v_1^T}=\begin{bmatrix} 0\\0\\1\\0\end{bmatrix} \cdot 3\cdot\begin{bmatrix} 0&0&0&1\end{bmatrix}=\begin{bmatrix} 0&0&0&0\\0&0&0&0\\0&0&0&3\\0&0&0&0\end{bmatrix}$, 所以由$\mathbf{u_i}\sigma_i\mathbf{v_i^T},i=1,2,3$得到的三个`Rank One`矩阵将$\bf A$中的$3,2,1$都筛选了出来, 所以:
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025048143.png)

> 现在我们把$\bf A$的最后一行拿掉，得到：
> $\bf A=\begin{bmatrix} 0&1&0&0\\0&0&2&0\\0&0&0&3\end{bmatrix}$, $\bf A^TA=\begin{bmatrix} 0&0&0&0\\0&1&0&0\\0&0&4&0\\0&0&0&9\end{bmatrix}$,$\bf AA^T=\begin{bmatrix} 1&0&0\\0&4&0\\0&0&9\end{bmatrix}$，此时$\bf AA^T$的第四行第四列消失了，但其所有的非零特征值还都在。
> 此时$\bf U=\begin{bmatrix} 0&0&1\\0&1&0\\1&0&0 \end{bmatrix}$, $\bf \Sigma=\begin{bmatrix} 3&0&0&0\\0&2&0&0\\0&0&1&0 \end{bmatrix}$, $\bf V=\begin{bmatrix} 0&0&0&1\\0&0&1&0\\0&1&0&0\\1&0&0&0 \end{bmatrix}$
> 我们发现，将$\bf A_{4\times 4}$移除最后一行变为$\bf A_{3\times 4}$, 相应的$\bf U$的最后一行和列被移除，$\bf V$不变。这说明了`SVD`可以适应所有的长方形矩阵。



## 5.2 应用
### 5.2.1 类似于Excel中的`V-Lookup`
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025049545.png)


### 5.2.2 统计词频
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025055334.png)



# 6 奇异值和特征值比较
## 6.1 谱分解比较
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025058538.png)



## 6.2 奇异值和特征值排序
> 本小节给出**求任意矩阵的最大奇异值的方法**。如果我们求出了矩阵的最大奇异值，那么我们就可以利用[秩1展开](https://www.yuque.com/alexman/so5y8g/psybsc/edit#yPGSz)得到原矩阵的最主要的信息，也就是后续要介绍的图像压缩的应用。
> 由于矩阵$\bf A$是任意形状的，所以他的二次型通常不是一个碗状图像(读者可以自行验证)，但是我们可以转而先求对称矩阵$\bf S=A^TA$的最大特征值， 然后看看这个最大的特征值和最大的奇异值之间有什么微妙的联系

 

### 6.2.1 最大特征值与奇异值
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025053621.png)
> 1. **我们从求**$\bf S=A^TA$**的最大特征值开始**
> 
由于$\bf Sx=\lambda x$两边同乘$\bf x^T$, 得到$\bf \lambda = \frac{x^TSx}{x^Tx}$, 我们将$\lambda$视为$\bf x$的向量函数$\bf r:R^n\to R$, 得到瑞利商的表达式:$\bf r(x)=\frac{x^TSx}{x^Tx}$
> 我们对$\bf r(x)$对求偏导$\bf \frac{\partial r}{\partial x}=0$, 计算过程较为复杂，这里直接给出结果:
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025056772.png)
> 2. **有了最大的特征值**$\lambda_1=r_{max}(\mathbf{x})$**, 接下来我们求**$\bf A$**的最大奇异值**
> 
![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025056170.png)
> 所以最大化$\frac{\bf ||Ax||}{\bf ||x||}$得到的奇异向量$\bf x=v_1$和使得$\bf S$的特征值最大的$\bf x$是一样的。

 


### 6.2.2 第二大的特征值和奇异值
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025057852.png)
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025062336.png)



## 6.3* 快速计算S的特征值和A的奇异值
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025064833.png)
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025063896.png)



## 6.4* 特征值和奇异值的数值稳定性
> 看下面的例子:
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025063632.png)
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025078198.png)



## 6.5 总结
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025071437.png)



# 7 Rayleigh Quotient
## Definition
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025078460.png)



## Extremity
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025079267.png)

**Proof with Induction - One at a time**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025078735.png)

# 8 图像处理中的应用
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025088619.png)
> 本节我们以图像处理为主题
> 对于一个$m\times n$的矩阵$\bf A$, 其每一项取值为$0$或$1$, 我们可以得到一个灰度图。
> 假设现在$\bf A$的每一项都是$1$
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025086151.png)
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025081149.png)
> 总之是为了加快计算速度，减小内存开销。
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025081644.png)


# 9 练习
## P1 计算奇异值分解**⭐⭐**
> 找到矩阵$\bf C=\begin{bmatrix} 5&5\\-1&7\end{bmatrix}$的奇异值分解。

**Key**首先这个矩阵没有什么特殊性质，他非奇异，也不对称，我们就老老实实求$\bf A^TA$和$\bf AA^T$
首先求$\bf A^TA$, $\begin{bmatrix} 5&-1\\5&7\end{bmatrix}\begin{bmatrix} 5&5\\-1&7\end{bmatrix}=\begin{bmatrix} 26&18\\18&74\end{bmatrix}$， 然后求$\bf A^TA$的特征值
$\lambda^2-100\lambda+1600=0$, 所以$\lambda_1=20,\lambda_2=80$, 于是$\bf \Sigma=\begin{bmatrix} 2\sqrt{5}&0\\0&4\sqrt{5}\end{bmatrix}$
接着求对应的正交特征向量$\bf v_1=\begin{bmatrix} -3/\sqrt{10}\\1/\sqrt{10}\end{bmatrix}$, $\bf v_2=\begin{bmatrix} 1/\sqrt{10}\\3/\sqrt{10}\end{bmatrix}$, 然后求$\bf u_1,u_2$
$\bf u_1=Av_1=\begin{bmatrix} -\sqrt{10}\\\sqrt{10}\end{bmatrix}$, $\bf u_2=Av_2=\begin{bmatrix} 2\sqrt{10}\\2\sqrt{10}\end{bmatrix}$, 注意我们这里还需要对$\bf u_i$做标准化操作， 除以其长度
于是$\bf U=\begin{bmatrix} -1/\sqrt{2}&1/\sqrt{2}\\1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}$, $\bf V=\begin{bmatrix} -3/\sqrt{10}&1/\sqrt{10}\\1/\sqrt{10}&3/\sqrt{10}\end{bmatrix}$
于是$\bf A=U\Sigma V^T$


## P2 斐波那契数列**⭐⭐⭐**
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025082772.png)

**Key**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025085625.png)


## P3 有正交列的矩阵的SVD**⭐⭐**
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025088484.png)

**Key**首先$\bf A=\begin{bmatrix} \bf w_1&\bf w_2&\cdots &\bf w_n\end{bmatrix}$, 因为其各列正交，所以$\bf A^TA=\begin{bmatrix} \mathbf{w_1^Tw_1}&&&\\&\bf w_2^Tw_2&&\\&&\ddots&\\&&&\bf w_n^Tw_n\end{bmatrix}=\begin{bmatrix} \sigma_1^2&&&\\&\sigma_2^2&&\\&&\ddots&\\&&&\sigma_n^2\end{bmatrix}$
于是我们知道对于一个对角矩阵来说，他是对称矩阵，所以他可以正交对角化，所以$\bf A^TA=V\Sigma V^T=I\Sigma I=\begin{bmatrix} \sigma_1^2&&&\\&\sigma_2^2&&\\&&\ddots&\\&&&\sigma_n^2\end{bmatrix}$, 所以$\bf V=I$(因为我们的目的是通过$\bf A^TA$这个矩阵求出$\bf V$)
所以$\bf U'=AV=\begin{bmatrix} \bf w_1&\bf 
 w_2&\cdots &\bf w_n\end{bmatrix}$
然后根据分解步骤，接下来我们需要对$\bf U'$的列做正交化。
于是我们有$\bf U=\begin{bmatrix} \frac{\mathbf{w_1}}{\sigma_1}& \frac{\mathbf{w_n}}{\sigma_n}\cdots & \frac{\mathbf{w_n}}{\sigma_n}\end{bmatrix}$,$\bf V=I$且$\bf \Sigma =\begin{bmatrix} \sigma_1^2&&&\\&\sigma_2^2&&\\&&\ddots&\\&&&\sigma_n^2\end{bmatrix}$


## P4 Important Facts**⭐⭐⭐⭐**
### Eigenvalues of AA^T&A^TA
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025085244.png)

**Solution**$\mathbf{A=U\Sigma V^T}$, $\mathbf{A^T=V\Sigma^TU^T}$, 利用$\mathbf{V^TV=I}$, 所以

- $\mathbf{AA^T=U\Sigma V^TV\Sigma^T U^T=U\Sigma\Sigma^TU^T}$
- $\mathbf{A^TA=V\Sigma^T U^TU\Sigma V^T=V\Sigma^T\Sigma V^T}$

假设我们的`SVD`是`Full SVD`, 则此时的$\mathbf{\Sigma}$是$\begin{bmatrix} \bf \Sigma_{r\times r}&\bf 0\\\bf 0&\bf 0_{(m-r)\times (n-r)}\end{bmatrix}$, 而我们通过分块矩阵的乘法可知，$\mathbf{\Sigma^T\Sigma}$和$\mathbf{\Sigma\Sigma^T}$的非零对角线元素是完全相同的，都是$\sigma_{11}^2$to $\sigma_{rr}^2$(如果$Rank(\mathbf{A})=k$), 对应的是$\mathbf{A^TA}$的$k$个非零特征值$\lambda_{11}$to $\lambda_{rr}$。

### Eigenvectors of AA^T&A^TA
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025095772.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025095630.png)

### Trace of A^TA = Sum of Max-K Singular Value
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025095188.png)

**(a)**假设$\mathbf{A}_{m\times n}$的列是$a_i$, 则$\mathbf{Tr(A^TA)}=a_1^Ta_1+a_2^Ta_2+\cdots +a_n^Ta_n$, 而$a_1^Ta_1=||a_1||_2^2$, 所以$\mathbf{Tr(A^TA)}=\sum_{i=1}^n ||a_i||_2^2=\sum_{i,j}a_{i,j}^2$
**(b)**假设$\mathbf{A}$是`Rank-One Matrix`, 则$\mathbf{A^TA}$也是`Rank One Matrix`(因为$\mathbf{A^TA}$和$\mathbf{A}$的秩相同)，则$\mathbf{A^TA}$有一个非零实特征值$\lambda_1=\sigma_1^2$。而我们知道$\mathbf{Tr(A^TA)}=\lambda_1$(矩阵的迹等于特征值之和，可通过特征多项式证明)，且根据$(a)$问我们知道$\mathbf{Tr(A^TA)}=\sum_{i,j}a_{ij}^2$。
所以综上，$\sigma_1^2 = \sum_{i,j}a_{ij}^2$
推广到一般，我们有$\sum_{i,j}a_{ij}^2=\sum_{i=1}^k \sigma_i^2$, $\sigma_1, \cdots, \sigma_k$是$k$个非零的奇异值。


### Largest Singular Value
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025095501.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025096732.png)

## P5 If A is invertible?
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025094499.png)

**Solution**假设是`Full SVD`, 则$\mathbf{U,V,\Sigma}$都是方阵。此时$\mathbf{VV^T=V^TV=U^TU=UU^T=I}$, 即$\mathbf{(V^T)^{-1}}=\mathbf{V}$, $\mathbf{U^{-1}=U^T}$, 于是$\mathbf{A^{-1}=(V^T)^{-1}\Sigma^{-1}U^{-1}=V\Sigma^{-1}U^T}$, 此时$\mathbf{A^{-1}}$的奇异值就是$\frac{1}{\sigma_{i}}$
$\mathbf{A^{-1}A=V\Sigma^{-1}U^TU\Sigma V^T=I}$

## P6 Matrix Factorization
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025093592.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025093347.png)![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025103082.png)


## P7 Construct SVD
### From Linear Mapping
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025101966.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025104457.png)


### From Orthogonal Columns⭐⭐⭐⭐⭐
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025108554.png)

**Solution - Reduced SVD**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025108814.png)
本题中我们还可以知道: 
$\mathbf{Rank(A^TA)=Rank(A)=n}$, 此时我们使用`Reduced SVD`可知$\mathbf{U\in \mathbb R^{m\times n}}$, $\mathbf{V,\Sigma\in \mathbb R^{n\times n}}$, 所以$\mathbf{A\Sigma^{-1}}$是合法的。


### From QR
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025105080.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025105380.png)
Orthogonal times Orthogonal = Orthogonal 使用矩阵代数即可证明


### From Symmetric Matrix⭐⭐⭐⭐⭐
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025103992.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025105862.png)

## P8 Matrix Approximation⭐⭐⭐⭐⭐
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025105120.png)
> 本题非常重要，在`18.065`中会深入讲解`Eckart-Young Theorem`

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025118127.png)

## P9 Isometric Matrices
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025115442.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025114078.png)


## P10 Rayleigh Quotient⭐⭐⭐⭐⭐
### Maximum
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025116976.png)
> 本题使用结论: $\frac{x^TSx}{x^Tx}=\lambda_{max}(S)$, 证明在[Proof of Maximum Value of Rayleigh Quotient](https://www.yuque.com/alexman/so5y8g/qnb3pv82tgwy10ng/edit#qK64Y)中给出

**(a) Largest Eigenvalue**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025117421.png)
**(b) Largest Singular Value**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025115243.png)


### Minimum
> ![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025119030.png)

**Solution**![image.png](./3.5_奇异值分解(SVD).assets/20230302_2025118650.png)
## 
