> 本章节我们首先将介绍一个无参估计问题，也就是不会对总体的分布做任何假设的估计方法。目标是估计`CDF`函数$F$。然后，我们将估计`Statistical Functionals`(也就是`CDF`的函数，比如均值，方差和协方差)。无参估计`Functionals`也被称为`Plug-in Method`。


# 1 Empirical Cumulative Distribution Function
[EDF.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/12393765/1664113288113-0aecfce3-e483-4dba-9ba3-c3b6212ccd0e.pdf)
## 1.1 ECDF Defintion
> 为了估计$F_X(x)$, 假设我们有$n$个数据点，我们只需要统计所有小于$x$的数据点的频率即可，也就是$F_X(x)=\mathbb{P}(X\leq x)\approx\frac{\#\{X_i\leq x\}}{n}=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x\}=F_n(x)$, 正如下面的定义:
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915348476.png)
> $F_{n,X}$也经常作为$F_X(x)$的估计量。一般写作$F_n(X)$
> **下面是一些常用记号:**
> - $F_{n}(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x\}$, $\hat{F_{n}}(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{x_i\leq x\}$(注意$X_i$和$x_i$的区别)
> - 关于$F_n(x)$的理解，我们可以将$F_n(x)$看成是一系列伯努利随机变量的和，也可以将$F_n(x)$看成是一个`CDF`, 这个`CDF``puts mass`$\frac{1}{n}$`at each point`$X_i$。
> - 关于$\hat{F_n}(x)$的理解，我们可以将$\hat{F_n}(x)$看成是一系列伯努利随机变量的和，也可以将$\hat{F_n}(x)$看成是一个`CDF`, 这个`CDF``puts mass`$\frac{1}{n}$`at each point`$x_i$。

**Nerve Data**![image.png](./__经验函数和插入估计量.assets/20231117_0915353337.png)


## 1.2 Python实现ECDF
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915365002.png)
假定我们的数据是$2.1,1.3, 3.4,2.5,1.7$, 则我们一般会经过下列步骤:

1. 对数据进行从小到大排列, 得到$1.3,1.7,2.1,2.5,3.4$, 所以$\hat{F_n}(3)=\frac{4}{5}=0.8$
2. 分类讨论$x$的值得到`ECDF`:

$\hat{F_n}(x)=\begin{cases} 0&x<1.3\\0.2&1.3\leq x<1.7\\ 0.4& 1.7\leq x <2.1 \\ 0.6&2.1\leq x< 2.5\\ 0.8&2.5\leq x<3.4\\1&x\geq 3.4\end{cases}$， 这是一个`Step Function`, 使用Python代码可视化如下
![image.png](./__经验函数和插入估计量.assets/20231117_0915364531.png)
可以发现$\hat{F_n}(x)$确实是将$\frac{1}{n}$的概率分配到了每一个数据点$X_i$上。
:::
```r
# 封装
def ecdf(data):
    x=np.array([-10000,*np.sort(data),10000])
    y=np.array([0,*np.arange(1,len(data)+1)/len(data),1])
    plt.xlim(np.min(x[1:]-1),np.max(x[:-1]+1))
    plt.ylim(-0.1,1.1)
    plt.ylabel("Empirical Probability")
    plt.xlabel("Data Points")
    plt.title("Empirical Distribution Function")
    plt.step(x,y,marker=".", where="post")
ecdf(np.array([1.3,1.7,2.1,2.5,3.4]))
```

## 1.3 Graphical Interpretations
### 1.3.1 n=5
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915371631.png)
:::

### 1.3.2 n=100/1000
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915382333.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915383012.png)
:::

### 1.3.3 Why EDF works?
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915398981.png)
:::



## 1.4 ECDF的性质
### 1.4.1 均值方差
> $\mathbb{E}[F_n(x)]=F_X(x)$
> $\mathbb{V}(F_n(x))=\frac{F(x)(1-F(x))}{n}$
> **均值方差的由来(伯努利分布):**
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915403361.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915415239.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915423438.png)
> Then:
> $\mathbb{E}[F_n(x)]=\mathbb{E}[\frac{1}{n}\sum_{i=1}^n I(X_i\leq x)]=\frac{1}{n}\sum_{i=1}^nE[I(X_i\leq x)]=\frac{1}{n}\cdot n\cdot F_X(x)=F_X(x)$
> $\mathbb{V}(F_n(x))=\frac{\sum_{i=1}^n Var(X_i)}{n^2}=\frac{F(x)(1-F(x))}{n}$

**算例**![image.png](./__经验函数和插入估计量.assets/20231117_0915436836.png)

### 1.4.2 MSE/偏差/Consistency
> 由`1.4.1`中可知，$\hat{F_n}(x)$是一个无偏估计量。
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915432130.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915442184.png)



### 1.4.3 依概率收敛于标准高斯分布
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915458827.png)
:::
**Proof**By intuition, $\sqrt{n}(\hat{F_n}(x)-F_X(x))\rightsquigarrow N(0,F_X(x)(1-F_X(x)))$. Then we will use central limit theorem to show it.
We know that $\hat{F_n}(x)$ is the sum of $n$ independent Bernoulli R.V. with parameter $F_X(x)$ and also we have $\begin{cases} E[\hat{F_n}(x)]=E[1\{X_i\leq x\}]=F_X(x)\\Var(\hat{F_n}(x))=\frac{Var(Y_i)}{n^2}=\frac{F_X(x)(1-F_X(x))}{n} \end{cases}$. 
Thus by central limit theorem, we have $\frac{\sqrt{n}(\hat{F_n}(x)-F_X(x))}{\sqrt{F_X(x)(1-F_X(x))}}\rightsquigarrow N(0,1),n\to \infty$, which simply implies $\sqrt{n}(\hat{F_n}(x)-F_X(x))\rightsquigarrow N(0,F_X(x)(1-F_X(x)))$(By the property of variance), which ends our proof.


### 1.4.4 协方差
>  Let $s$ and $t$ be two distinct points. Find the covariance and correlation of $F_n(s)$ and $F_n(t)$.
> By definition, we have:
> $\begin{aligned}F_n(s)&=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq s\}\\F_n(t)&=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq t\}\\\mathbf{1}\{X_i\leq s\}&\sim Bernoulli(F_X(s))\\\mathbf{1}\{X_i\leq t\}&\sim Bernoulli(F_X(t))\end{aligned}$
> By definition of covariance, we have:
> $\begin{aligned} Cov(\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq s\},\frac{1}{n}\sum_{j=1}^n \mathbf{1}\{X_j\leq t\})&=\frac{1}{n^2}[n\cdot Cov(\mathbf{1}\{X_i\leq s\},\mathbf{1}\{X_i\leq t\})\nonumber\\&+(n^2-n)Cov(\mathbf{1}\{X_i\leq s\},\mathbf{1}\{X_j\leq t\})]\tag{By definition}\end{aligned}$
> We know that when $i=j$:
> $\begin{aligned} Cov(\mathbf{1}\{X_i\leq s\},\mathbf{1}\{X_i\leq t\})&=E[\mathbf{1}\{X_i\leq s\}]\cdot E[\mathbf{1}\{X_j\leq t\}]-E[\mathbf{1}\{X_i\leq s\}]\cdot E[\mathbf{1}\{X_j\leq t\}]\\&=F_X(min\{s,t\})-F_X(s)F_X(t)\tag{By definition}\end{aligned}$
> and that when $i\neq j$:
> $\begin{aligned} Cov(\mathbf{1}\{X_i\leq s\},\mathbf{1}\{X_j\leq t\})&=E[\mathbf{1}\{X_i\leq s\}\cdot \mathbf{1}\{X_i\leq t\}]-E[\mathbf{1}\{X_i\leq s\}]\cdot E[\mathbf{1}\{X_i\leq t\}]\\&=F_X(s)F_X(t)-F_X(s)F_X(t)=0\end{aligned}$
> By definition of correlation, we have:
> $\begin{aligned} \rho(\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq s\},\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq t\})&=\frac{Cov(\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq s\},\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq t\})}{\sqrt{Var(\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq s\})\times Var(\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq t\})}}\\&=\frac{\frac{1}{n}[F_X(min\{s,t\})-F_X(s)F_X(t)]}{\sqrt{\frac{1}{n^2}F_X(s)(1-F_X(s))F_X(t)(1-F_X(t))}} \\&=\frac{F_X(min\{s,t\})-F_X(s)F_X(t)}{\sqrt{F_X(s)(1-F_X(s))F_X(t)(1-F_X(t))}}\end{aligned}$



### 1.4.5 总结
> **定义：**
> $F_n(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x\}$
> **性质:**
> - **无偏：**
> 
$E[F_n(x)]=F_X(x)$, 无偏。
> - **MSE收敛:**
> 
$Var(F_n(x))=F_X(x)(1-F_X(x))$, 	$E[(F_n(x)-F_X(x))^2]=Var(F_n(x))=\frac{F_X(x)(1-F_X(x))}{n}\to 0, n\to \infty$
> - **依概率收敛：**
> 
$F_n(x)\stackrel{P}\to F_X(x)$
> - **收敛于高斯分布(CLT):**
> 
![image.png](./__经验函数和插入估计量.assets/20231117_0915451842.png)
> **分布:**
> $F_n(x)\sim Bernoulli(F_X(x))$
> **样本实现, 部署计算:**
> $\hat{F_n}(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{x_i\leq x\}$



## 1.5 Multivariate ECDF
### 1.5.1 Definition
:::info
$F_{n}(x,y)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x, Y_i\leq y\}$
$\widehat{F}_{n}(x,y)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{x_i\leq x, y_i\leq y\}$, 
![image.png](./__经验函数和插入估计量.assets/20231117_0915465150.png)[https://stats.stackexchange.com/questions/263877/how-to-compute-bivariate-empirical-distribution](https://stats.stackexchange.com/questions/263877/how-to-compute-bivariate-empirical-distribution)
:::

### 1.5.2 Unbiased/Consistent
> **证明**`**Unbiasedness**`**:**
> 我们令$Z_i=\mathbf{1}\{X_i\leq x,Y_i\leq y\}$, $Z_i=\begin{cases}1&X_i\leq x,Y_i\leq y\\0&otherwise \end{cases}$
> $\begin{cases}\mathbb{P}(Z_i=1)=\mathbb{P}(X_i\leq x, Y_i\leq y)=F_{XY}(x,y) \\ \mathbb{P}(Z_i=0)=\mathbb{P}(X_i\leq x, Y_i\leq y)=1-F_{XY}(x,y)\end{cases}$
> $\begin{aligned}\mathbb{E}[F_n(x,y)]&=\frac{1}{n}\mathbb{E}[\sum_{i=1}^n \mathbf{1}\{X_i\leq x, Y_i\leq y\}]\\&=1\cdot F_{XY}(x,y)+0\cdot (1-F_{XY}(x,y))=F_{XY}(x,y)\end{aligned}$, 于是$F_{n}(x,y)$是无偏的。
> **证明**`**Consistent**`**:**
> 我们知道$F_n(x,y)$是由一系列独立同分布地服从$Bernoulli(F_{XY}(x,y))$的变量的和的平均值构成的，于是根据大数定律, 我们知道$\begin{aligned}\lim_{n\to \infty}\mathbb{P}(|F_n(x,y)-F_{XY}(x,y)|>\epsilon)=0\end{aligned}$, 于是$F_n(x,y)$实际上是一个`Consistent Estimator`。



### 1.5.3 MSE Convergence/Mean/Variance
> **类比Univariate ECDF中的推导，我们知道:**
> - $E[F_n(x,y)]=F_{XY}(x,y)$, 对应其无偏性
> - $Var[F_n(x,y)]=F_{XY}(x,y)(1-F_{XY}(x,y))$, 可用于证明其`MSE Convergence`
> - $MSE(F_n(x,y))=\frac{F_{XY}(x,y)(1-F_{XY}(x,y))}{n}$
> 
**其实我们可以利用**$F_n(x,y)$**的方差表达式证明其是**`**MSE Converge**`**的，证明如下:**$\begin{aligned}\lim_{n\to \infty}E[{F}_n(x,y)-F_{XY}(x,y)]&=\lim_{n\to \infty}\mathbb{V}({F}_n(x,y))=\lim_{n\to \infty}\frac{F_{XY}(x,y)(1-F_{XY}(x,y))}{n}\to 0\end{aligned}$



### 1.5.4 Marginal ECDF
> 我们知道对于一个`CDF`来说，$\lim_{y\to \infty}F_{xy}(x,y)=F_{X}(x)$, 那么对于`ECDF`是否也会有这个性质呢?
> 我们知道$F_{n,XY}(x,y)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x, Y_i\leq y\}$, 且$\begin{cases} \mathbb{P}(\mathbf{1}\{X_i\leq x,Y_i\leq y\}=1)=F_{XY}(x,y)\\\mathbb{P}(\mathbf{1}\{X_i\leq x,Y_i\leq y\}=0)=1-F_{XY}(x,y)\end{cases}$
> 当$y\to \infty$时，$\begin{cases} \mathbb{P}(\mathbf{1}\{X_i\leq x,Y_i\leq y\}=1)=\lim_{n\to\infty}F_{XY}(x,y)=F_X(x)=\mathbb{P}(\mathbf{1}\{X_i\leq x\}=1)\\\mathbb{P}(\mathbf{1}\{X_i\leq x,Y_i\leq y\}=0)=1-\lim_{n\to \infty}F_{XY}(x,y)=1-F_X(x)=\mathbb{P}(\mathbf{1}\{X_i\leq x\}=0)\end{cases}$
> 于是$\lim_{y\to \infty}F_{n,XY}(x,y)=\lim_{y\to \infty}\frac{1}{n}\sum_{i=1}^n\mathbf{1}\{X_i\leq x, Y_i\leq y\}=\frac{1}{n}\sum_{i=1}^n\mathbf{1}\{X_i\leq x\}= F_{n,X}(x)$, 证毕。



## 1.6 ECDF的置信区间
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915474798.png)


## 1.7 Glivenko-Cantellu定理
> 根据大数定理可知，当试验次数增大时，事件的频率稳定于概率。那么，当试验次数增大时，表示事件 出现概率的总体分布函数呢？这个问题可由格利文科定理来回答。(Uniform LLN)
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915485447.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915495055.png)
> 这个定理和`Statistical Learning Theory`有关，`Excess Risk(Bias and Variance)`, 会用到这个定理。
> **用于控制机器学习模型的**`**Generalization Error**`**。**



# 2 Plug-in Estimators and Statistical Functionals
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915496706.png)
> 很多时候，我们可能不仅仅想估算$F$的表达式，而是一些$F$的函数，比如均值，方差，或者中位数。换句话说，如果要估计参数$\theta=T(F)$, 我们只需要计算$T(\hat{F_n})$即可，$\hat{\theta_n}=T(\hat{F_n})$也被称为`Plug-in Estimate`。
> `Plug-in Principle`是一种非常拥有的，用于构建`Consistent Estimator`的原则。 



## 2.1 Plug-in Principle
[ECDF-Histogram.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/12393765/1664155312832-95d3b54b-b912-4246-9175-d0ff4eddc0c5.pdf)
:::info
前文我们已经介绍并证明了`ECDF`对于$F_X(x)$来说是一个`Consistent Estimator`, 由这个性质，我们有如下应用。
![image.png](./__经验函数和插入估计量.assets/20231117_0915507383.png)
其中$dF(x)=f(x)dx$（其实就是连续型变量的`Probability Mass`）， 常见的$r(x)$包括:

- `Population mean`: $r(x)=x$
- `Higher Moment`: $r(x)=x^p$

而$T(F)$之所以被称为是`Linear Functional`如下：
**在**$f(x),g(x)$**是连续的概率密度函数的情况下：**
$\begin{aligned} T(aF+bG)&=\int r(x)d(aF(x)+bG(x))\\&=\int r(x)[af(x)+bg(x)]dx\\&=\int ar(x)f(x)dx+br(x)g(x)dx\\&=a\int  r(x)dF(x)+b\int r(x)dG(x)\\&=aT(F)+bT(G) \end{aligned}$
**在**$f(x),g(x)$**是离散的概率质量函数的情况下:**
$\begin{aligned} T(aF+bG)&=\sum_i r(x_i)[af(x_i)+bg(x_i)]\\&=\sum_iar(x_i)f(x_i)+br(x_i)g(x_i)\\&=a\sum_ir(x_i)f(x_i)+b\sum_Ir(x_i)g(x_i)\\&=aT(F)+bT(G)\end{aligned}$
这也印证了$T$具有线性性质，是一个线性函数。
:::



## 2.2 Plug-in Estimator's Understanding
> 假设我们要估计总体的某个参数$\theta$: $\theta=\mathbb{E}[r(X)]=\int_{\mathbb{R}}r(x)dF_X(x)$, 且我们手头有$X_1,X_2,\cdots, X_n\sim F$, 我们怎构建我们的`Estimator`$\hat{\theta}$呢? 其实思想很简单，根据`Plug-in Principle`我们只要把参数中的$F_X(x)$全部替换成$\hat{F_n}(x)$即可。
> 具体来说，于是我们会如下选择`Plug-in Estimator`:
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915516043.png)
> **怎么理解这个积分**$\hat{\theta_n}=\int_{\mathbb{R}}r(x)d\hat{F_n}(x)$**呢?**
> 首先，从前文我们知道$\hat{F_n}(x)$是一个离散的函数，于是$\int_{\mathbb{R}}r(x)d\hat{F_n}(x)=\sum_{i=1}^{n} r(x_i)d\hat{F_n}(x_i)$
> 其次，$\hat{F_n}(x_i)$是一个`Step Function`, 我们从[Step and Delta](https://www.yuque.com/alexman/dydxis/lgbk8g#jlVTx)中知道，`Step Function`的一阶导数是`Delta Function`, 所以$d\hat{F_n}(X_i)=\hat{\delta_n}(X_i)$, 且$\hat{\delta_n}(X_i)=\frac{1}{n},X_i=x_i$
> 所以$\sum_{i=1}^n r(X_i)d\hat{F_n}(x_i)=\sum_{i=1}^nr(X_i)\hat{\delta_n}(x)=\frac{1}{n}\sum_{i=1}^n r(X_i)$, 证毕。
> **为什么**$\hat{F_n}(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x\}$**本身也是一个**`**CDF**`**?**
> 对于$\hat{F_n}(x)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x\}$来说，它本身也是一个`CDF`, 描述了一个离散随机变量的`CDF`, 且每一个数据点的概率都是$\frac{1}{n}$
> 假设$X_i$都是不同的， 那么$F_n(x)$is the CDF of a discrete R.V. $X^{*}$$\mathbb{P}(X*=x_i)=\frac{1}{n}$

**Remarks**以上所有的`Estimator`$\hat{\theta}=T(\hat{F_n}(x))$都是在$T$是`Linear Functional`的条件下才使用的。

## 2.3 Common Plug-in Estimators
### 2.3.1 Estimating Mean
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915528288.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915539950.png)
:::
**Proof of unbiasedness and consistency**， 所以是`Unbiased Estimator`
根据切比雪夫不等式：, 所以是`Consistent Estimator`


### 2.3.2 Estimating Variance
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915546550.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915551104.png)
于是我们的`Plug-in Estimator`给出的方差估计量其实不是无偏的(详见[样本方差无偏性](https://www.yuque.com/alexman/kziggo/bxxml7#Pgv5M)), 同时是`Consistent`的，证明在下面给出。
![image.png](./__经验函数和插入估计量.assets/20231117_0915562082.png)
:::
**Proof of asymptotically unbiasedness and consistency****首先它是**`**Biased**`**的:**
因为， 所以
**但是它又是**`**Asymptotically unbiased**`**的:**
因为当, 
**它又一定是**`**Consistent**`**的:**
因为, 所以

### 2.3.3 Estimating quantiles
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915565396.png)![image.png](./__经验函数和插入估计量.assets/20231117_0915579371.png)
> 如果我们写得更清晰一些：
> 对于总体来说(`population quantile`):$q_{\alpha}=\inf_{t}\{t:F_X(t)\geq \alpha\}$
> 对于样本，我们不知道$F_X(t)$， 所以只能使用`Plug-in Estimator`来估计: $\hat{q_{\alpha}}=\inf_{t}\{t:\hat{F_n}(t)\geq \alpha\}$
> $\hat{q_{\alpha}}\stackrel{P}{\longrightarrow} q_{\alpha}$, 下面我们将给出证明。

**Proof of biasedness but consistency**根据定义$\hat{q}_{\alpha}=\inf_{t}\{t:\widehat{F}_n(t)\geq \alpha\}=\inf_{t}\{t:\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq t\}\geq \alpha\}$


### 2.3.4 Estimating Median
:::info
![image.png](./__经验函数和插入估计量.assets/20231117_0915581159.png)
:::
**Proof of consistency**


### 2.3.5 Estimating Skewness
> ![image.png](./__经验函数和插入估计量.assets/20231117_0915595059.png)

**Proof of Consistency**


### 2.3.6 Estimating Covariance/Correlation
> 现在假设我们收集到了`2D Samples`(`Random Vectors`) $\{(X_i,Y_i)\}_{i=1}^n$, 其中$(X_1,Y_1),(X_2,Y_2),\cdots, (X_n,Y_n)$互相独立，换句话说:
> $Cov(X_i,Y_j)=\begin{cases} Cov(X_i,Y_j)=\sigma_{XY}&i=j\\0&i\neq j\end{cases}$。
> $E[XY]=\mu_{XY}, E[X]=\mu_X,E[Y]=\mu_Y$
> $\sigma_{XY}=Cov(X,Y)=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]=\mu_{XY}-\mu_X\mu_Y$, 如果$X=Y$, $\sigma_{XY}=Var(X)$。那么我们如何利用二维数据来估算$\sigma_{XY}$呢?
> **我们只需要使用**`**2D ECDF**`**即可，我们给出它的定义:**
> 给定数据样本:$\{(X_i, Y_i)\}_{i=1}^n$, 我们有定义$\hat{F_{n}}(x,y)=\frac{1}{n}\sum_{i=1}^n \mathbf{1}\{X_i\leq x,Y_i\leq y\}$(也是一个均值的概念, 可以想象一个$\frac{\#\space  in\space rectangle}{n}$), 这是一个对于$F_{X,Y}(x,y)=\mathbb{P}(X\leq x, Y\leq y)$的估计量。
> **所以, 我们有:**
> - $\sigma_{XY}=\int_{\mathbb{R^2}}xy\space dF(x,y)-\int_{\mathbb{R}}x\space dF(x,y)\int_{\mathbb{R}}y\space dF(x,y)$
> - $\hat{\sigma_{XY}}=\int_{\mathbb{R^2}}xy\space d\hat{F_n}(x,y)-\int_{\mathbb{R}}x\space d\hat{F_n}(x,y)\int_{\mathbb{R}}y\space d\hat{F_n}(x,y)=(\frac{1}{n}\sum_{i=1}^nX_iY_i)-\bar{X_n}\bar{Y_n}$
> 
和一维的例子类似，我们有$\mathbb{P}(X^*=x_i, Y^*=y_i)=\frac{1}{n}$, given that the random vectors $(X^*,Y^*)$are distinct.
> **Correlation: **$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$
> **下面我们证明: **$\widehat{\sigma}_{XY} = \frac{1}{n} \sum_{i=1}^n X_iY_i - \overline{X}_n \overline{Y}_n=\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X}_n)(Y_i-\overline{Y}_n)$
> $\begin{aligned}\frac{1}{n}\sum_{i=1}^n (X_i-\overline{X}_n)(Y_i-\overline{Y}_n)&=\frac{1}{n}\sum_{i=1}^n(X_iY_i-X_i\overline{Y}_n-\overline{X}_nY_i+\overline{X}_n\overline{Y}_n)\\&=\frac{1}{n}\sum_{i=1}^{n}X_iY_i-\frac{2}{n}\cdot n\overline{X}_n\overline{Y}_n+\overline{X}_n\overline{Y}_n\\&=\frac{1}{n} \sum_{i=1}^n X_iY_i - \overline{X}_n \overline{Y}_n \end{\aligned}$, 证毕。

**Proof of Biased But Consistency****我们有:**
**证明Biased:**

We know that . So the bias: , which isn't equal to zero. Thus  is biased.
**证明Consistency:**
We know by `WLLN` that , and . By continuous mapping theorem, we can construct a continuous function . Since we have , . Thus . Therefore, we are safe to conclude that  is a consistent estimator for the covariance.


# 3 Histogram in detail
> 前文我们介绍了使用`ECDF`来估算一个随机变量$X$的`CDF`$F_X(x)$, 且我们证明了这是一个不错的估计。但是这种估计方式有局限性。
> 假设我们有一系列样本点$X_1,\cdots,X_n$，来自一个连续的概率密度函数$f(\cdot)$。如果我们仍然使用`ECDF`来估计`CDF`, 那么因为`ECDF`$\hat{F_n}(x)$是一个离散的函数，所以即便我们使用`ECDF`估算出了`CDF`$F_X(x)$, 我们在试图对其求导$f_X(x)=\frac{d}{dx}F_X(x)=F_X'(x)$时就会遇到一些困难，因为他不连续。
> 为了克服这个困难，我们有一系列的策略。因为$\hat{F_n}\stackrel{P}{\longrightarrow}F, n\to \infty$, 而且我们知道$F$是连续函数，所以其实$F$的导数存在，因此我们可以转而估计$F$的导数。



## 3.1 Quantiles
> 对于数据集$S=\{x_1,x_2,\cdots, x_n\}$来说($x_i$是$X$的实现)，我们定义`Quantiles`为$n-1$个数据点$\hat{q_1},\hat{q_2},\cdots, \hat{q_{n-1}}\in S$, 使得:
> $P(X\leq \hat{q_1})=P(\hat{q}_1\leq X\leq \hat{q_2})=\cdots=P(\hat{p}_{n-2}\leq X\leq \hat{p}_{n-1})=P(X\geq \hat{p}_{n-1})$
> 且:
> $F_X(\hat{q}_i)=\frac{i}{n}, i=1,2,\cdots, n-1$
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916009017.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916024566.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916032962.png)



## 3.2 Histogram for Continuous Data
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916042023.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916056034.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916067524.png)


## 3.3 Estimate PDF
### EPDF
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916065875.png)
> 这里对于任意数据点$x_i$的`Empirical Probability`我们有$f_{hist}(t)dt$其中$dt=\frac{l}{b}$, 于是$P(X=t)=\frac{1}{n}$。

**Proof of the validity - Axiom**![image.png](./__经验函数和插入估计量.assets/20231117_0916071906.png)


### Choice of Width
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916082340.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916096373.png)



# 4 Kernel Density Estimation
## Motivation
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916117424.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916127139.png)



## Definition
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916121654.png)
> 可以将`Gaussian Kernel`理解成一个$N(x_i, \frac{1}{2}h)$的分布。

**Graphical Interpretations - Fig 3.15**![image.png](./__经验函数和插入估计量.assets/20231117_0916139965.png)


## Rectangular Kernel
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916137469.png)



## Gaussian Kernel
### Properties
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916149260.png)
> 下面是一些`Scaling anmd Shifting`性质:
> 1. **Original**
> 
![image.png](./__经验函数和插入估计量.assets/20231117_0916147094.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916144704.png)
> 2. Shift by $\mu$, 使得$\tilde{y}=\tilde{x}+\mu$即$\mu(\tilde{y})=\mu$
> 
![image.png](./__经验函数和插入估计量.assets/20231117_0916157534.png)
> 3. Scale by $\frac{1}{\sigma}$, get a normalized gaussian random variable $\frac{z-\mu}{\sigma}\sim \mathcal{N}(0,1)$, don't forget to normalize it across bandwidth.
> 
![image.png](./__经验函数和插入估计量.assets/20231117_0916165553.png)![image.png](./__经验函数和插入估计量.assets/20231117_0916172108.png)


### Code Implementations
```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from scipy.stats import gaussian_kde

seed = 0
rng = np.random.RandomState(seed)
n = 10000

def plot_kde_truth(ax, x, label):
    kde = gaussian_kde(x)
    x_sorted = np.sort(x)
    ax.plot(np.sort(x_sorted), kde(x_sorted), label=label)

def gaussian_kernel(dist):
    return 1 / np.sqrt(2 * np.pi) * np.exp(-dist**2 / 2)

x = rng.normal(size=n)

h = 0.5
a_range = np.linspace(x.min(), x.max(), 100)

kde_estimates = [gaussian_kernel((a - x) / h).sum() / (n * h) for a in a_range]
fig, ax = plt.subplots(1, 1)
ax.plot(a_range, kde_estimates, label="Estimate")
plot_kde(ax, x, "Ground truth")
ax.legend()
```
> ![image.png](./__经验函数和插入估计量.assets/20231117_0916176130.png)

