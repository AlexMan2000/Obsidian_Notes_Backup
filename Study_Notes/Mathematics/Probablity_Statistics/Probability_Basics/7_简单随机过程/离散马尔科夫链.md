# 0 Preface
> ![image.png](离散马尔科夫链.assets/20231101_1702469706.png)
>  我们会探究一种模型，他将过去的影响总结成一个`State`, 且这个`State`根据给定的概率随时间变化。本章我们将这个模型限制在只能取有限数量的状态，且状态转移的概率和时间是独立的。我们想要探究`State Value`序列的概率性质。
> 本章描述的模型类型的应用范围非常广的。它包括几乎所有包含不确定性的动力系统，只要系统的**状态**被适当地定义。这样的系统出现在各种各样的领域，例如通信，自动控制，信号处理。制造、经济学和行为研究。



# 1 Discrete Time Markov Chain
## 1.0 前言
> 我们从离散伯努利分布出发来引出马尔科夫链的概念，假设我们有如下的伯努利序列:
> ![image.png](离散马尔科夫链.assets/20231101_1702461066.png)
> 则我们有如下的状态和其转移概率。
> ![image.png](离散马尔科夫链.assets/20231101_1702464658.png)
> 下面我们将正式介绍马尔科夫过程。




## 1.1 标准定义
> ![image.png](离散马尔科夫链.assets/20231101_1702478761.png)
> 随机变量$X_n$描述了马尔科夫状态，样本空间是离散的$S=\{1,\cdots m\}$($m$是正整数), 称为状态空间。马尔科夫链由状态转移概率$p_{ij}$组成，表示从状态$i$变成状态$j$的概率。



## 1.2 Markov Property**⭐⭐⭐⭐⭐**
:::info
马尔科夫链的一个重要性质/假设就是:
![image.png](离散马尔科夫链.assets/20231101_1702496695.png)
这个性质说的是，我知道了在$n$时刻我在状态$i$, 那么在$n$时刻之前我的状态序列对我在$n+1$时刻我的状态没有任何关系。也就是我不关心我是怎么在$n$时刻到达状态$i$的。
:::


## 1.3 n-step transition probability**⭐⭐⭐**
### 1.3.1 Definition
:::info
我们知道，在马尔科夫过程进行状态转移$i\to j$的过程中，有可能我们经历了一步，也可能经历了很多步。现在我们定义:
使用$n$步从状态$i$到状态$j$的概率是: $P_{ij}^{(n)}=P\{X_{m+n}=j|X_m=i\}$
:::


### 1.3.2 Recursion Formula
> 那么我们怎么计算这个`n-step transition probability`呢?
> 因为从状态$i$出发所能到达的状态$j$是可以列举出来的，所以$\sum_{j}P_{ij}^{(n)}=\sum_{j}P(X_n=j|X_0=i)=1$(因为$P$是一个合法的概率密度函数)。
> $n-step\space transition\space probability$探究的是从某个状态$i$开始, **在**$n$**次状态转移之后**的状态为$j$的概率。
> 我们可以从第$n$次状态转移开始反向考虑建立递归关系，如下图所示:
> ![image.png](离散马尔科夫链.assets/20231101_1702511781.png)
> 意思就是我经过了$n-1$步有可能转移到$1,2,...,m$这$m$种状态，用$k$表示具体转移到的某种状态，其概率是$P_{ik}^{(n-1)}$, 然后从第$n-1$步由状态$k$转移到第$n$步的状态$j$的概率是$p_{kj}$, 根据乘积法则相乘得到经过$n$步状态转移从$i\to k\to j$的转移概率， 然后将所有的$\to k\to j$的转移概率相加得到$r_{ij}(n)$, 就有了下面的递归公式:
> ![image.png](离散马尔科夫链.assets/20231101_1702527363.png)![image.png](离散马尔科夫链.assets/20231101_1702538895.png)
> 上面的推导是基于`Initial State`是状态$i$的情况，那么如果我们考虑所有的初始状态，则有:
> ![image.png](离散马尔科夫链.assets/20231101_1702558045.png)
> 根据全概率公式: $P(X_n=j)=\sum_{i=1}^m P(X_n=j|X_0=i)P(X_0=i)=\sum_{i=1}^m P(X_0=i)r_{ij}(n)$。




### 1.3.3 Chapman-Kolmogorov Equations
:::info
![image.png](离散马尔科夫链.assets/20231101_1702563495.png)
:::
**Proof**![image.png](离散马尔科夫链.assets/20231101_1702573179.png)


### 1.3.4 Probability of a Path⭐⭐⭐
> ![image.png](离散马尔科夫链.assets/20231101_1702589916.png)
> 总的来说我们只要沿着`Path`将沿途的所有状态转移概率相乘即可。
> ![image.png](离散马尔科夫链.assets/20231101_1703005577.png)
> $P(X_1=2,X_2=6,X_3=7|X_0=1)=p_{12}p_{26}p_{67}$
> $P(X_4=7|X_0=2)=p_{26}p_{67}p_{76}p_{67}+p_{26}p_{66}p_{66}p_{67}+p_{21}p_{12}p_{26}p_{67}$
> 但是如果$X_n$中的$n$很大的话，用这种方式会得到非常长的概率连乘，此时我们会转而使用$1.3$中的`Recursion Formula`来求解



## 1.4 Matrix Perspective**⭐⭐⭐⭐⭐**
### Markov Matrix
> 上面的推导略显繁琐，我们在矩阵的框架下进行阐述。
> ![image.png](离散马尔科夫链.assets/20231101_1703019599.png)
> 其中$P_{ij}=P(X_{m+1}=j|X_{m}=i)$, 也就是`One-step transition probability`。
> 令$P=\begin{bmatrix} P_{11}&P_{12}&\cdots &P_{1m}\\P_{21}&P_{22}&\cdots&P_{2m}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}&P_{n2}&\cdots&P_{mm}\end{bmatrix}$, 则我们想证明$P^n=\begin{bmatrix} P_{11}^{(n)}&P_{12}^{(n)}&\cdots &P_{1m}^{(n)}\\P_{21}^{(n)}&P_{22}^{(n)}&\cdots&P_{2m}^{(n)}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}^{(n)}&P_{m2}^{(n)}&\cdots&P_{mm}^{(n)}\end{bmatrix}$
> 我们使用数学归纳法: 
> 1. **Base Step: **$n=1$, 因为$P_{ij}^{(1)}=P_{ij}$by definition, 表示`1-step transition probability`
> 2. **Inductive Hypothesis: **假设$n=k\geq 1$, $P^k=\begin{bmatrix} P_{11}^{(k)}&P_{22}^{(k)}&\cdots &P_{1m}^{(k)}\\P_{21}^{(k)}&P_{22}^{(k)}&\cdots&P_{2m}^{(k)}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}^{(k)}&P_{m2}^{(k)}&\cdots&P_{mm}^{(k)}\end{bmatrix}$成立。
> 3. **Inductive Step:** 对于$P^{k+1}$中的一项$P_{ij}^{(k+1)}$, 根据`Chapman-Kolmogorov Equation`可知: $P_{ij}^{(k+1)}=\sum_{k=1}^mP_{ik}^{(k)}P_{kj}$, 而$\sum_{k=1}^mP_{ik}^{(k)}P_{kj}$表示的正是$P^k$中的第$i$行和$P$中第$j$列向量的乘积。
> 
所以$\begin{aligned}P^{k+1}=P^kP&=\begin{bmatrix} P_{11}^{(k)}&P_{22}^{(k)}&\cdots &P_{1m}^{(k)}\\P_{21}^{(k)}&P_{22}^{(k)}&\cdots&P_{2m}^{(k)}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}^{(k)}&P_{m2}^{(k)}&\cdots&P_{mm}^{(k)}\end{bmatrix}\begin{bmatrix} P_{11}&P_{22}&\cdots &P_{1m}\\P_{21}&P_{22}&\cdots&P_{2m}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}&P_{n2}&\cdots&P_{mm}\end{bmatrix}\\&=\begin{bmatrix} P_{11}^{(k+1)}&P_{22}^{(k+1)}&\cdots &P_{1m}^{(k+1)}\\P_{21}^{(k+1)}&P_{22}^{(k+1)}&\cdots&P_{2m}^{(k+1)}\\\vdots&\vdots&\ddots&\vdots\\P_{m1}^{(k+1)}&P_{m2}^{(k+1)}&\cdots&P_{mm}^{(k+1)}\end{bmatrix}\end{aligned}$, 成立。
> 证毕。



### State Probability Distribution
> 假设我们有一个初始状态向量$\vec{\pi}_0=\begin{bmatrix} \pi_0(1)\\\pi_0(2)\\\vdots\\\pi_0(m)\end{bmatrix}\in \mathbb{R}^{m}$，状态空间是$\{1,2,\cdots, m\}$(也就是有$m$个状态)。
> 其中$\sum_{k=1}^m\pi_0(k)=1$, $\pi_0(k)$表示在`Time 0`的时候我们位于状态$k$的概率大小。
> 对于任意时刻$k$, $\vec{\pi}_k=\begin{bmatrix} \pi_k(1)\\\pi_k(2)\\\vdots\\\pi_k(m)\end{bmatrix}\in \mathbb{R}^{m}$, $\pi_k(i)=P(X_{k}=i)$, 表示在第$k$个时间步到达State k 的概率。
> 根据全概率公式我们有:
> $\begin{aligned}\pi_k(j)&=\sum_{i=1}^mP(X_k=j|X_{k-1}=i)P(X_{k-1}=i)\\&=\sum_{i=1}^mP(X_k=j|X_{k-1}=i)\pi_{k-1}(i)\\&=\sum_{i=1}^m\pi_{k-1}(i)P_{ij}\end{aligned}$
> 所以:
> $\vec{\pi}_{k+1}^{\top}=\pi_{k}^{\top}P$
> 所以我们有:
> $\pi_n^{\top}=\pi_0^{\top}P^n,n\geq 0$







# 2 Classification of States
## Recurrent/Transient States⭐⭐⭐⭐⭐
### Definition 1: Verbal
> [!def]
> The first key insight about Markov chains is that, while some states will be visited by the chain over and over again, other states will only be visited a few times and then never seen again.
> 
> 主要有两类: `Recurrent`和`Transient`
> 
> ![image.png](离散马尔科夫链.assets/20231101_1703025549.png)
> 
> 总的来说，`recurrent class`就是一个`class`进去了就出不去了。



### Definition 2: Hitting Time
> [!def]
> For each $x ∈ X$ , the random variable $T_x$ represents the first time that the chain visits state $x$, i.e., $T_x := min\{n ∈ N : X_n = x\}$ (this is called the hitting time of state $x$). 
> 
> We will also need the random variable $T_x^+:=min\{n ∈ Z_+ : X_n = x\}$ , which is the hitting time for state $x$ except that we do not let $T_x^+$ equal $0$ when the chain starts at $x$.
> 
> Also, the notations $P_x$ and $E_x$ mean that the chain is started at state $x$, that is:
> ![](离散马尔科夫链.assets/image-20240204110552134.png)![](离散马尔科夫链.assets/image-20240204110654945.png)![](离散马尔科夫链.assets/image-20240204110752371.png)![](离散马尔科夫链.assets/image-20240204133755595.png)![](离散马尔科夫链.assets/image-20240204133818688.png)

> [!example]
> ![](离散马尔科夫链.assets/image-20240204133854869.png)![](离散马尔科夫链.assets/image-20240204134521931.png)![](离散马尔科夫链.assets/image-20240204134529769.png)


### Finite DTMC has at least one recurrent state
> [!important]
> ![](离散马尔科夫链.assets/image-20240204134718175.png)


### Optional: Positive/Null Recurrence
> [!def]
> ![](离散马尔科夫链.assets/image-20240204150945348.png)![](离散马尔科夫链.assets/image-20240204150954513.png)




## Irreducibility
### Accessible States
> [!def]
> ![](离散马尔科夫链.assets/image-20240204140922551.png)



### Communicating Class
> [!def]
> ![](离散马尔科夫链.assets/image-20240204135216626.png)
> That is to say, if we view the markov chain as a directed graph, then irreducibility means there is only one SCC in the di-graph.
> 
> 


### Irreducible Markov Chain
> [!def]
> ![](离散马尔科夫链.assets/image-20240204141202696.png)![](离散马尔科夫链.assets/image-20240204141258850.png)




## Periodicity
### Periodic States
> [!def]
> ![](离散马尔科夫链.assets/image-20240204141752953.png)
> When d > 1, state i is periodic.
> When d = 1, state i is aperiodic.

> [!example]
> ![](离散马尔科夫链.assets/image-20240204141802543.png)![](离散马尔科夫链.assets/image-20240204141811327.png)

> [!bug] Important
> ![](离散马尔科夫链.assets/image-20240204141822663.png)
> So normally we assume all states in our markov chain are aperiodic(having period 1).


###  Aperiodic Markov Chain
> [!def]
> A Markov chain is said to be aperiodic if all of its states are aperiodic.
> ![](离散马尔科夫链.assets/image-20240204142523586.png)![](离散马尔科夫链.assets/image-20240204152438716.png)

> [!proof] Proof Sketch: Aperiodicity means d=1 for all states
> ![](离散马尔科夫链.assets/image-20240204162153980.png)![](离散马尔科夫链.assets/image-20240204162147432.png)


### Check Aperiodicity
> [!lemma] 
> All states in an irreducible Markov Chain have the same period.

> [!thm]
> ![](离散马尔科夫链.assets/image-20240204142436702.png)





### Periodic Class⭐⭐
> [!def]
> A class is said to be periodic if its states are periodic.
> ![](离散马尔科夫链.assets/image-20240204152507922.png)![](离散马尔科夫链.assets/image-20240204152520803.png)
> 
> `Periodicity`描述了一个`Reucurrent Class`内部的马尔科夫过程的状态转移方式。
> 对于一个`Recurrent Class`来说：
> ![image.png](离散马尔科夫链.assets/20231101_1703123819.png)
> $d=1\space or\space 2$**的情况:**
> ![image.png](离散马尔科夫链.assets/20231101_1703137132.png)
> $d=3$**的情况**
> ![image.png](离散马尔科夫链.assets/20231101_1703158153.png)
> 再看一个特殊的例子，对于下列的`Recurrent Class`:
> ![image.png](离散马尔科夫链.assets/20231101_1703163523.png)
> 红色的为$S_1$,白色的是$S_2$, $S_1$和$S_2$是`Disjoint`的， 所以这个`Recurrent Class`是周期性的。

**更严格的Periodicity的定义**![image.png](离散马尔科夫链.assets/20231101_1703176587.png)



### Examples
> [!example]
> ![](离散马尔科夫链.assets/image-20240204162552321.png)![](离散马尔科夫链.assets/image-20240204162602107.png)






## Recurrent/Transient Class
### Definition
> [!def]
> A communicating class is called recurrent if at least one (equivalently, every) state in this class is recurrent. 
> 
> A communicating class is transient if at least one (equivalently, every) state in this class is transient.
> ![](离散马尔科夫链.assets/image-20240204135614663.png)![](离散马尔科夫链.assets/image-20240204135727757.png)![](离散马尔科夫链.assets/image-20240204151015812.png)




### Recurrent and Transient Class Example
> [!example]
> ![](离散马尔科夫链.assets/image-20240204140428801.png)![](离散马尔科夫链.assets/image-20240204140436856.png)![](离散马尔科夫链.assets/image-20240204140444521.png)![](离散马尔科夫链.assets/image-20240204140452076.png)![](离散马尔科夫链.assets/image-20240204140504569.png)![](离散马尔科夫链.assets/image-20240204140511317.png)





## Markov Decomposition
> [!def]
> 
> ![image.png](离散马尔科夫链.assets/20231101_1703056367.png)
> 上图中，状态$5,6,7,8$都是`Recurrent`的。
> 状态$1,2,3,4$都是`Transient`的(因为$1\to 5$或者$2\to 6$总是有可能会发生，导致我们永远无法回到$1,2,3,4$)。
> 于是我们可以将这个马尔科夫过程中的状态分成两类:
> 蓝色的部分是`Recurrent Class`, 红色的部分是`Transient Class`, 一旦马尔科夫过程进入了`Recurrent Class`, 就卡在里面出不去了，和其他的`Class`之间就没有交流的通道了。
> ![image.png](离散马尔科夫链.assets/20231101_1703089729.png)
> 所以如果我们从一个`Recurrent State`开始马尔科夫过程，我们在$n\to \infty$步之后会收敛在`Recurrent Class`中。而如果我们从一个`Transient State`开始马尔科夫过程，我们不会收敛在`Transient Class`中。这也进一步验证了`Initial State`的选取确实会对马尔科夫的收敛性与否产生影响。
> 这种分类被称为`Markov Decomposition`:
> ![image.png](离散马尔科夫链.assets/20231101_1703094312.png)
> ![image.png](离散马尔科夫链.assets/20231101_1703102262.png)
> 我们再看一个例子:
> ![image.png](离散马尔科夫链.assets/20231101_1703126242.png)

> [!example]
> ![image.png](离散马尔科夫链.assets/20231101_1703047180.png)

> [!example] Decomposing a Markov Chain with Transition Matrix
> ![](离散马尔科夫链.assets/image-20240204144002222.png)![](离散马尔科夫链.assets/image-20240204144008393.png)



# 3 Steady State Analysis
> [!important] When we have stationary distribution
> 
> When the markov chain is irreducible and aperiodic, we have stationary distribution.

## Stationarity of Sequential R.V.
> [!def]
> ![](离散马尔科夫链.assets/image-20240204144116736.png)


## Initial Distribution
> [!def]
> ![](离散马尔科夫链.assets/image-20240204155942266.png)![](离散马尔科夫链.assets/image-20240204155926564.png)




## Invariant Distribution
> [!def]
> 对于一个`Single Aperiodic Class`的`Markov Process`来说，我们可以使用`Balance Equation`$\begin{cases}\pi=\pi P\\\sum\limits_{i=1}^n\pi_i=1\end{cases}$, 求出收敛概率, 其中$P$是`Transition Matrix`, 每一行的和为$1$。
> 
> 如果`Transition Matrix` 的每一列和为一，则`Balance Equation` 是 $P\pi=\pi$
> ![](离散马尔科夫链.assets/image-20240204160004079.png)![](离散马尔科夫链.assets/image-20240204160042948.png)![](离散马尔科夫链.assets/image-20240204160054028.png)










## Steady-State Convergence Theorem
### Theorem 1: N-Step Transition Probability
> [!thm]
> ![](离散马尔科夫链.assets/image-20231118182346859.png)![](离散马尔科夫链.assets/image-20231118182350646.png)![](离散马尔科夫链.assets/image-20231118182358973.png)
> 例如, 在$n\to\infty$时，`Markov Chain`已经收敛，此时$P(X_0=j)=\pi_j$, 则:
>  $\begin{align}P(X_2=j)&=\sum\limits_{t=1}^m\sum\limits_{k=1}^mP(X_2=j|X_1=k,X_0=t)P(X_1=k|X_0=t)P(X_0=t)\\&=\sum\limits_{t=1}^{m}\sum\limits_{k=1}^{m}p_{kj}\times p_{tk}\times\pi_t\\&=\sum\limits_{k=1}^{m}p_{kj} \sum\limits_{t=1}^{m}p_{tk}\times\pi_t\\&=\sum\limits_{k=1}^mp_{kj}\pi_k\\&=\pi_j\end{align}$ 
>  以此类推可以证明$P(X_n=j)=\pi_j$
>  所以$\lim_{n\to \infty}P(X_n=j|X_0=i)=\lim_{n\to \infty}P(X_n=j)=\pi_j$, 即$X_n$和$X_0$在$n\to \infty$时近似独立。

> [!example] 
> ![](离散马尔科夫链.assets/image-20240204155208392.png)![](离散马尔科夫链.assets/image-20240204155234644.png)





### Theorem 2: Portion of Time Converges
> [!thm]
> ![](离散马尔科夫链.assets/image-20240204151329044.png)
> Note that this convergence doesn't rigorously hold since the LHS is a random variable and the RHS is a constant. But we won't go into the detail of this.



### Theorem 3: Expected Proportion of Time 
> [!thm]
> ![](离散马尔科夫链.assets/image-20240204153905531.png)![](离散马尔科夫链.assets/image-20240204153913795.png)

> [!corollary] 
> ![](离散马尔科夫链.assets/image-20231118174405725.png)![](离散马尔科夫链.assets/image-20231118174416654.png)




### Theorem 4: Hitting Time Version
> [!thm]
> ![](离散马尔科夫链.assets/image-20240204145332661.png)

> [!proof]
> ![](离散马尔科夫链.assets/image-20240204150412566.png)![](离散马尔科夫链.assets/image-20240204150419316.png)![](离散马尔科夫链.assets/image-20240204150428145.png)![](离散马尔科夫链.assets/image-20240204150459888.png)![](离散马尔科夫链.assets/image-20240204150507278.png)![](离散马尔科夫链.assets/image-20240204150714332.png)





## Existence and Uniqueness of Stationary Dsitribution
### Existence
> [!thm]
> ![](离散马尔科夫链.assets/image-20240204151149123.png)


### Uniqueness
> [!thm]
> You should just be aware that an irreducible, aperiodic, finite state Markov Chain has exactly one stationary distribution.
> 
> **This is particularly helpful if you happen to guess a solution to the balance equations.** If the solution that you have guessed is a probability distribution, you have found the stationary distribution of the chain.
> ![](离散马尔科夫链.assets/image-20240204151447792.png)![](离散马尔科夫链.assets/image-20240204151500429.png)


### Examples
> [!example]
> ![](离散马尔科夫链.assets/image-20240204151941070.png)![](离散马尔科夫链.assets/image-20240204151949172.png)![](离散马尔科夫链.assets/image-20240204151955423.png)


> [!example]
> ![](离散马尔科夫链.assets/image-20240204152018736.png)




## Periodicity and Stability
### Definition
> [!important]
> ![](离散马尔科夫链.assets/image-20240204153306303.png)



### Theorem 4: Aperiodic Convergence
> [!thm]
> ![](离散马尔科夫链.assets/image-20240204153313107.png)![](离散马尔科夫链.assets/image-20240204160539801.png)
> The fact that periodic markov chain's stationary distribution depends on initial distribution and initial states, like in the following example:
> 
> ![](离散马尔科夫链.assets/20231101_1703089729.png)
> where we have many communicating classes(SCC)。
> 
> This means if we start at the left SCC, we will get stuck in there and find an invariant distribution for that SCC. The same holds if we starts at the right SCC. 
> 
> Here is another example:
> ![](离散马尔科夫链.assets/20231101_1703158153.png)
> Depending on our initial states, the stationary distribution will be different.





### Perron-Frobenius Theorem
[Stochastic Matrices](../../../../Computer_Science/Machine_Learning/EECS127AB/1_Linear_Algebra_Review/Structured_Matrices.md#Stochastic%20Matrices)




## Birth-Death Process
### Definition
> [!def]
> ![](离散马尔科夫链.assets/image-20231118174432513.png)

### Suumary
> [!summary]
> ![](离散马尔科夫链.assets/image-20231118180325768.png)
> 如果$\rho=1$, 则每个`State`的`Steady State Probability`是Equally likely的。
> 如果$\rho<1$且$m\to \infty$, 则：$p_{X_n}(i)=\rho^i(1-\rho)$, 所以$\mathbb{E}[X_n]=\sum\limits_{k=0}^{\infty}k\times (1-\rho)\times \rho^k=\frac{\rho}{1-\rho}$, 当$\rho\to 0$时，$\mathbb{E}[X_n]$是`Finite`的，但是当$\rho\to 1$时，图像就朝着`Infinity`进发了。



## Phone Call Process
> [!summary]
> ![](离散马尔科夫链.assets/image-20231118193413140.png)


# 4 Reversibility





# 5 Markov Chain Monte Carlo
## Absorption Probability
> [!def]
> ![](离散马尔科夫链.assets/image-20231118194932115.png)![](离散马尔科夫链.assets/image-20231118194941228.png)![](离散马尔科夫链.assets/image-20231118195013670.png)
> 更进一步，我们有$a_i=P(A|X_n=i),\forall n\in \mathbb{N}$
> 
> 

> [!example]
> ![](离散马尔科夫链.assets/image-20231118195428923.png)


## Expected Time to Absorption
> [!def]
> ![](离散马尔科夫链.assets/image-20231118195553121.png)![](离散马尔科夫链.assets/image-20231118195559342.png)

> [!example]
> ![](离散马尔科夫链.assets/image-20231118195609827.png)



## Mean First Passage and Recurrence Time
> [!def]
> ![](离散马尔科夫链.assets/image-20231118200308163.png)![](离散马尔科夫链.assets/image-20231118200321849.png)![](离散马尔科夫链.assets/image-20231118200331291.png)

> [!example]
> ![](离散马尔科夫链.assets/image-20231118200342991.png)![](离散马尔科夫链.assets/image-20231118200348929.png)








# 6 Applications
## Diffusion of Gas Particles
> [!example]
> ![](离散马尔科夫链.assets/image-20240204164708259.png)![](离散马尔科夫链.assets/image-20240204164712893.png)

> [!Solution]
> ![](离散马尔科夫链.assets/image-20240204164728208.png)![](离散马尔科夫链.assets/image-20240204164759800.png)

> [!code]
> ![](离散马尔科夫链.assets/image-20240204164829541.png)![](离散马尔科夫链.assets/image-20240204164841770.png)
```python
N = 100

states = np.arange(N+1)

def transition_probs(i, j):
    if j == i:
        return 1/2
    elif j == i+1:
        return (N-i)/(2*N)
    elif j == i-1:
        return i/(2*N)
    else:
        return 0

ehrenfest = MarkovChain.from_transition_function(states, transition_probs)
Plot(ehrenfest.steady_state(), edges=True)
```



## Hidden Markov Model
> [!example]
> ![](离散马尔科夫链.assets/image-20240204165103843.png)![](离散马尔科夫链.assets/image-20240204165334923.png)![](离散马尔科夫链.assets/image-20240204165344637.png)





